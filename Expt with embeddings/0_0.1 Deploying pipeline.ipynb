{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from bs4  import BeautifulSoup\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#Lemmatizing and POSTagging\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "modalVerb =['can', 'could', 'may', 'might', 'will', 'would', 'shall', 'should', 'must']\n",
    "stopWords = set(stopwords.words('english'))\n",
    "stopWords.remove('not')\n",
    "stopWords.remove('can')\n",
    "stopWords.remove('will')\n",
    "  \n",
    "\n",
    "tqdm.pandas() \n",
    "\n",
    "import pickle\n",
    "from keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_path = \"C:/Program Files/Java/jdk1.8.0_201/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "#os.environ['STANFORD_POSTAGGER'] = \"C:/Users/Me/Documents/stanford-postagger-full-2018-10-16\"\n",
    "os.environ['STANFORD_POSTAGGER'] = \"D:/NLP/Library file/stanford-postagger-full-2018-10-16\"\n",
    "Stanford_postagger = StanfordPOSTagger(os.environ['STANFORD_POSTAGGER'] +'/models/english-bidirectional-distsim.tagger',os.environ['STANFORD_POSTAGGER']+'/stanford-postagger.jar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IN MAIN AS WELL AS DEPLOYABLE SCRIPT\n",
    "INPUT_TEXT_LENGTH = 294 \n",
    "\n",
    "PROJ_NAME= \"1.0 Glove_pretrained_100dims_20k_LSTM\"\n",
    "MODEL_FILEPATH = f'model_tokenizers_weights_and_json/{PROJ_NAME}_best_model.json'\n",
    "WEIGHT_FILEPATH = f'model_tokenizers_weights_and_json/{PROJ_NAME}_best_weights.h5'\n",
    "TOKENIZER_FILEPATH = f'model_tokenizers_weights_and_json/{PROJ_NAME}_tokenizer_instance.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TO DO @ Deployable stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. preprocessing - total steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre process step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contraction(text):\n",
    "    contraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'can not'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'),\n",
    "                            (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),(r'(\\w+)\\'ve', '\\g<1> have'),\n",
    "                            (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), \n",
    "                            (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not'),\n",
    "                            (r'br', ' '),(r'www', ' '),\n",
    "                            (r'http\\S+', ' '),(r\"#(\\w+)\", ' '),(r\"@(\\w+)\", ' '),(r'[^\\w\\s]', ' ')                            \n",
    "                           ]\n",
    "\n",
    "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
    "    for (pattern, repl) in patterns:\n",
    "        (text, count) = re.subn(pattern, repl, text)\n",
    "    return text\n",
    "\n",
    "#can run spell checker\n",
    "# before punct removal of https <br> /n etc ..\n",
    "# not removed stop word\n",
    "#contradiction handling from below link\n",
    "# pos tag\n",
    "#lemmatization\n",
    "#find max_length to detect freq distribution\n",
    "\n",
    "def preProcess(text):\n",
    "    \n",
    "    soup = BeautifulSoup(text)# also added 'html.parser' as an argument to constructor\n",
    "    text =soup.get_text()\n",
    "    \n",
    "    tokens = word_tokenize(text) \n",
    "    #lowercase\n",
    "    tokens = [t.lower() for t in tokens]                                     #incas if its U.S untd state, IFB\n",
    "    # rmv that are not alphabet\n",
    "    tokens = [word for word in tokens if not word.isdigit()]\n",
    "    #rmv token that are of length 1\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    #rmv stopword\n",
    "    tokens = [word for word in tokens if word not in stopWords]\n",
    "    \n",
    "    return ' '.join(text for text in tokens if text not in punct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preProcess(replace_contraction(\"This movie really sucks! Can I get my money back please?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre process step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None \n",
    "    \n",
    "#tagged is defined globally\n",
    "\n",
    "def nltk_lemma(tag):\n",
    "    global tagged\n",
    "    lst=[]\n",
    "    for word, tag in tagged:\n",
    "        \n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        \n",
    "        if wntag is None:# not supply tag in case of None\n",
    "            lemma = nltk_lemmatizer.lemmatize(word)\n",
    "            lst.append(lemma)\n",
    "            #return \" \".join(lemma)\n",
    "            #print(lemma)\n",
    "        else:\n",
    "            lemma = nltk_lemmatizer.lemmatize(word, pos=wntag)\n",
    "            lst.append(lemma)\n",
    "            #return \" \".join(lemma)\n",
    "            #print(lemma)\n",
    "            \n",
    "    return lst\n",
    "\n",
    "\n",
    "def POS_lemmatize(review):\n",
    "    global tagged\n",
    "    #normal tagger\n",
    "    tagged = pos_tag(word_tokenize(review))\n",
    "    #stanford tagger\n",
    "    #tagged = Stanford_postagger.tag(word_tokenize(review))# imp word tokenize and repective POS\n",
    "    return \" \".join(nltk_lemma(tagged))# retuns setence after lemmatising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS_lemmatize(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing tokenizer & checking Vectorizing new inputs \n",
    "* store tokenizer @ training and retrive its values @ deployment \n",
    "* its working illustration 5.mlmastery @@ Vectorising KERAS - GRU.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.text.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "'''its working illustration 5.mlmastery @@ Vectorising KERAS - GRU.ipynb'''\n",
    "# in deployement script    \n",
    "#iniitialize it in the deployement script along with the .pickle file @ specified location\n",
    "with open(TOKENIZER_FILEPATH, 'rb') as handle:\n",
    "    prod_instance_tokenizer = pickle.load(handle)\n",
    "\n",
    "print(type(prod_instance_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie': 1,\n",
       " 'film': 2,\n",
       " 'one': 3,\n",
       " 'make': 4,\n",
       " 'like': 5,\n",
       " 'see': 6,\n",
       " 'get': 7,\n",
       " 'good': 8,\n",
       " 'time': 9,\n",
       " 'would': 10,\n",
       " 'character': 11,\n",
       " 'well': 12,\n",
       " 'go': 13,\n",
       " 'watch': 14,\n",
       " 'even': 15,\n",
       " 'story': 16,\n",
       " 'bad': 17,\n",
       " 'really': 18,\n",
       " 'think': 19,\n",
       " 'show': 20,\n",
       " 'scene': 21,\n",
       " 'great': 22,\n",
       " 'look': 23,\n",
       " 'much': 24,\n",
       " 'say': 25,\n",
       " 'end': 26,\n",
       " 'could': 27,\n",
       " 'know': 28,\n",
       " 'people': 29,\n",
       " 'also': 30,\n",
       " 'first': 31,\n",
       " 'give': 32,\n",
       " 'take': 33,\n",
       " 'way': 34,\n",
       " 'love': 35,\n",
       " 'act': 36,\n",
       " 'thing': 37,\n",
       " 'come': 38,\n",
       " 'play': 39,\n",
       " 'find': 40,\n",
       " 'life': 41,\n",
       " 'seem': 42,\n",
       " 'plot': 43,\n",
       " 'work': 44,\n",
       " 'actor': 45,\n",
       " 'two': 46,\n",
       " 'many': 47,\n",
       " 'year': 48,\n",
       " 'never': 49,\n",
       " 'want': 50,\n",
       " 'best': 51,\n",
       " 'little': 52,\n",
       " 'try': 53,\n",
       " 'ever': 54,\n",
       " 'man': 55,\n",
       " 'still': 56,\n",
       " 'part': 57,\n",
       " 'something': 58,\n",
       " 'director': 59,\n",
       " 'old': 60,\n",
       " 'back': 61,\n",
       " 'use': 62,\n",
       " 'funny': 63,\n",
       " 'lot': 64,\n",
       " 'real': 65,\n",
       " 'guy': 66,\n",
       " 'performance': 67,\n",
       " 'woman': 68,\n",
       " 'though': 69,\n",
       " 'feel': 70,\n",
       " 'star': 71,\n",
       " 'another': 72,\n",
       " 'big': 73,\n",
       " 'cast': 74,\n",
       " 'actually': 75,\n",
       " 'nothing': 76,\n",
       " 'new': 77,\n",
       " 'role': 78,\n",
       " 'young': 79,\n",
       " 'tell': 80,\n",
       " 'start': 81,\n",
       " 'write': 82,\n",
       " 'point': 83,\n",
       " 'every': 84,\n",
       " 'leave': 85,\n",
       " 'day': 86,\n",
       " 'world': 87,\n",
       " 'girl': 88,\n",
       " 'set': 89,\n",
       " 'horror': 90,\n",
       " 'turn': 91,\n",
       " 'minute': 92,\n",
       " 'comedy': 93,\n",
       " 'u': 94,\n",
       " 'long': 95,\n",
       " 'fact': 96,\n",
       " 'quite': 97,\n",
       " 'pretty': 98,\n",
       " 'action': 99,\n",
       " 'around': 100,\n",
       " 'kill': 101,\n",
       " 'however': 102,\n",
       " 'right': 103,\n",
       " 'enough': 104,\n",
       " 'line': 105,\n",
       " 'fan': 106,\n",
       " 'live': 107,\n",
       " 'may': 108,\n",
       " 'become': 109,\n",
       " 'happen': 110,\n",
       " 'must': 111,\n",
       " 'need': 112,\n",
       " 'series': 113,\n",
       " 'script': 114,\n",
       " 'music': 115,\n",
       " 'without': 116,\n",
       " 'original': 117,\n",
       " 'saw': 118,\n",
       " 'keep': 119,\n",
       " 'family': 120,\n",
       " 'always': 121,\n",
       " 'almost': 122,\n",
       " 'kid': 123,\n",
       " 'put': 124,\n",
       " 'do': 125,\n",
       " 'begin': 126,\n",
       " 'whole': 127,\n",
       " 'least': 128,\n",
       " 'enjoy': 129,\n",
       " 'believe': 130,\n",
       " 'last': 131,\n",
       " 'bit': 132,\n",
       " 'mean': 133,\n",
       " 'far': 134,\n",
       " 'place': 135,\n",
       " 'friend': 136,\n",
       " 'laugh': 137,\n",
       " 'lead': 138,\n",
       " 'effect': 139,\n",
       " 'kind': 140,\n",
       " 'reason': 141,\n",
       " 'might': 142,\n",
       " 'call': 143,\n",
       " 'anything': 144,\n",
       " 'since': 145,\n",
       " 'tv': 146,\n",
       " 'book': 147,\n",
       " 'let': 148,\n",
       " 'probably': 149,\n",
       " 'run': 150,\n",
       " 'child': 151,\n",
       " 'screen': 152,\n",
       " 'name': 153,\n",
       " 'away': 154,\n",
       " 'hard': 155,\n",
       " 'yet': 156,\n",
       " 'shot': 157,\n",
       " 'moment': 158,\n",
       " 'fun': 159,\n",
       " 'anyone': 160,\n",
       " 'help': 161,\n",
       " 'sure': 162,\n",
       " 'rather': 163,\n",
       " 'audience': 164,\n",
       " 'american': 165,\n",
       " 'idea': 166,\n",
       " 'war': 167,\n",
       " 'high': 168,\n",
       " 'dvd': 169,\n",
       " 'although': 170,\n",
       " 'especially': 171,\n",
       " 'read': 172,\n",
       " 'expect': 173,\n",
       " 'interest': 174,\n",
       " 'course': 175,\n",
       " 'everything': 176,\n",
       " 'job': 177,\n",
       " 'move': 178,\n",
       " 'maybe': 179,\n",
       " 'three': 180,\n",
       " 'sense': 181,\n",
       " 'mind': 182,\n",
       " 'worth': 183,\n",
       " 'different': 184,\n",
       " 'someone': 185,\n",
       " 'sound': 186,\n",
       " 'main': 187,\n",
       " 'problem': 188,\n",
       " 'night': 189,\n",
       " 'face': 190,\n",
       " 'true': 191,\n",
       " 'version': 192,\n",
       " 'money': 193,\n",
       " 'episode': 194,\n",
       " 'second': 195,\n",
       " 'everyone': 196,\n",
       " 'together': 197,\n",
       " 'follow': 198,\n",
       " 'black': 199,\n",
       " 'half': 200,\n",
       " 'instead': 201,\n",
       " 'hour': 202,\n",
       " 'special': 203,\n",
       " 'death': 204,\n",
       " 'short': 205,\n",
       " 'lose': 206,\n",
       " 'house': 207,\n",
       " 'recommend': 208,\n",
       " 'john': 209,\n",
       " 'eye': 210,\n",
       " 'talk': 211,\n",
       " 'wife': 212,\n",
       " 'beautiful': 213,\n",
       " 'head': 214,\n",
       " 'fall': 215,\n",
       " 'waste': 216,\n",
       " 'miss': 217,\n",
       " 'low': 218,\n",
       " 'later': 219,\n",
       " 'direct': 220,\n",
       " 'excellent': 221,\n",
       " 'classic': 222,\n",
       " 'hand': 223,\n",
       " 'budget': 224,\n",
       " 'boy': 225,\n",
       " 'father': 226,\n",
       " 'change': 227,\n",
       " 'else': 228,\n",
       " 'view': 229,\n",
       " 'production': 230,\n",
       " 'top': 231,\n",
       " 'piece': 232,\n",
       " 'fight': 233,\n",
       " 'viewer': 234,\n",
       " 'include': 235,\n",
       " 'nice': 236,\n",
       " 'poor': 237,\n",
       " 'remember': 238,\n",
       " 'release': 239,\n",
       " 'simply': 240,\n",
       " 'completely': 241,\n",
       " 'home': 242,\n",
       " 'picture': 243,\n",
       " 'men': 244,\n",
       " 'appear': 245,\n",
       " 'open': 246,\n",
       " 'less': 247,\n",
       " 'couple': 248,\n",
       " 'camera': 249,\n",
       " 'others': 250,\n",
       " 'understand': 251,\n",
       " 'die': 252,\n",
       " 'human': 253,\n",
       " 'along': 254,\n",
       " 'attempt': 255,\n",
       " 'word': 256,\n",
       " 'feature': 257,\n",
       " 'care': 258,\n",
       " 'dead': 259,\n",
       " 'hollywood': 260,\n",
       " 'school': 261,\n",
       " 'video': 262,\n",
       " 'song': 263,\n",
       " 'either': 264,\n",
       " 'wrong': 265,\n",
       " 'suppose': 266,\n",
       " 'stupid': 267,\n",
       " 'full': 268,\n",
       " 'title': 269,\n",
       " 'rest': 270,\n",
       " 'lack': 271,\n",
       " 'add': 272,\n",
       " 'next': 273,\n",
       " 'game': 274,\n",
       " 'early': 275,\n",
       " 'writer': 276,\n",
       " 'sex': 277,\n",
       " 'truly': 278,\n",
       " 'style': 279,\n",
       " 'small': 280,\n",
       " 'awful': 281,\n",
       " 'person': 282,\n",
       " 'interesting': 283,\n",
       " 'murder': 284,\n",
       " 'save': 285,\n",
       " 'meet': 286,\n",
       " 'sort': 287,\n",
       " 'mother': 288,\n",
       " 'terrible': 289,\n",
       " 'case': 290,\n",
       " 'review': 291,\n",
       " 'flick': 292,\n",
       " 'perhaps': 293,\n",
       " 'dialogue': 294,\n",
       " 'joke': 295,\n",
       " 'involve': 296,\n",
       " 'base': 297,\n",
       " 'mr': 298,\n",
       " 'wonderful': 299,\n",
       " 'age': 300,\n",
       " 'perfect': 301,\n",
       " 'close': 302,\n",
       " 'hope': 303,\n",
       " 'wonder': 304,\n",
       " 'create': 305,\n",
       " 'sequence': 306,\n",
       " 'comment': 307,\n",
       " 'bore': 308,\n",
       " 'guess': 309,\n",
       " 'often': 310,\n",
       " 'stop': 311,\n",
       " 'definitely': 312,\n",
       " 'light': 313,\n",
       " 'art': 314,\n",
       " 'killer': 315,\n",
       " 'drama': 316,\n",
       " 'heart': 317,\n",
       " 'side': 318,\n",
       " 'quality': 319,\n",
       " 'fine': 320,\n",
       " 'stand': 321,\n",
       " 'cut': 322,\n",
       " 'mention': 323,\n",
       " 'experience': 324,\n",
       " 'finally': 325,\n",
       " 'cinema': 326,\n",
       " 'yes': 327,\n",
       " 'absolutely': 328,\n",
       " 'oh': 329,\n",
       " 'certainly': 330,\n",
       " 'consider': 331,\n",
       " 'car': 332,\n",
       " 'example': 333,\n",
       " 'late': 334,\n",
       " 'white': 335,\n",
       " 'matter': 336,\n",
       " 'surprise': 337,\n",
       " 'force': 338,\n",
       " 'hit': 339,\n",
       " 'voice': 340,\n",
       " 'overall': 341,\n",
       " 'direction': 342,\n",
       " 'felt': 343,\n",
       " 'hear': 344,\n",
       " 'son': 345,\n",
       " 'entire': 346,\n",
       " 'several': 347,\n",
       " 'dark': 348,\n",
       " 'sit': 349,\n",
       " 'entertain': 350,\n",
       " 'hold': 351,\n",
       " 'type': 352,\n",
       " 'decide': 353,\n",
       " 'totally': 354,\n",
       " 'humor': 355,\n",
       " 'hero': 356,\n",
       " 'actress': 357,\n",
       " 'evil': 358,\n",
       " 'deal': 359,\n",
       " 'wait': 360,\n",
       " 'despite': 361,\n",
       " 'final': 362,\n",
       " 'ask': 363,\n",
       " 'buy': 364,\n",
       " 'already': 365,\n",
       " 'throughout': 366,\n",
       " 'number': 367,\n",
       " 'able': 368,\n",
       " 'unfortunately': 369,\n",
       " 'michael': 370,\n",
       " 'genre': 371,\n",
       " 'history': 372,\n",
       " 'god': 373,\n",
       " 'relationship': 374,\n",
       " 'daughter': 375,\n",
       " 'town': 376,\n",
       " 'dance': 377,\n",
       " 'support': 378,\n",
       " 'today': 379,\n",
       " 'walk': 380,\n",
       " 'shoot': 381,\n",
       " 'favorite': 382,\n",
       " 'credit': 383,\n",
       " 'rent': 384,\n",
       " 'city': 385,\n",
       " 'horrible': 386,\n",
       " 'power': 387,\n",
       " 'learn': 388,\n",
       " 'group': 389,\n",
       " 'wish': 390,\n",
       " 'pay': 391,\n",
       " 'stuff': 392,\n",
       " 'present': 393,\n",
       " 'score': 394,\n",
       " 'theme': 395,\n",
       " 'talent': 396,\n",
       " 'strong': 397,\n",
       " 'question': 398,\n",
       " 'past': 399,\n",
       " 'etc': 400,\n",
       " 'event': 401,\n",
       " 'behind': 402,\n",
       " 'level': 403,\n",
       " 'fail': 404,\n",
       " 'stay': 405,\n",
       " 'slow': 406,\n",
       " 'twist': 407,\n",
       " 'body': 408,\n",
       " 'situation': 409,\n",
       " 'chance': 410,\n",
       " 'blood': 411,\n",
       " 'soon': 412,\n",
       " 'obviously': 413,\n",
       " 'sometimes': 414,\n",
       " 'robert': 415,\n",
       " 'touch': 416,\n",
       " 'return': 417,\n",
       " 'order': 418,\n",
       " 'self': 419,\n",
       " 'please': 420,\n",
       " 'realize': 421,\n",
       " 'decent': 422,\n",
       " 'highly': 423,\n",
       " 'complete': 424,\n",
       " 'anyway': 425,\n",
       " 'illiant': 426,\n",
       " 'except': 427,\n",
       " 'forget': 428,\n",
       " 'speak': 429,\n",
       " 'figure': 430,\n",
       " 'monster': 431,\n",
       " 'country': 432,\n",
       " 'career': 433,\n",
       " 'rating': 434,\n",
       " 'documentary': 435,\n",
       " 'element': 436,\n",
       " 'police': 437,\n",
       " 'husband': 438,\n",
       " 'thought': 439,\n",
       " 'cool': 440,\n",
       " 'extremely': 441,\n",
       " 'novel': 442,\n",
       " 'violence': 443,\n",
       " 'simple': 444,\n",
       " 'reality': 445,\n",
       " 'drive': 446,\n",
       " 'pick': 447,\n",
       " 'crap': 448,\n",
       " 'ok': 449,\n",
       " 'hilarious': 450,\n",
       " 'hell': 451,\n",
       " 'effort': 452,\n",
       " 'state': 453,\n",
       " 'particularly': 454,\n",
       " 'annoy': 455,\n",
       " 'edit': 456,\n",
       " 'portray': 457,\n",
       " 'sister': 458,\n",
       " 'opinion': 459,\n",
       " 'david': 460,\n",
       " 'gore': 461,\n",
       " 'musical': 462,\n",
       " 'cause': 463,\n",
       " 'cop': 464,\n",
       " 'obvious': 465,\n",
       " 'room': 466,\n",
       " 'serious': 467,\n",
       " 'pace': 468,\n",
       " 'dream': 469,\n",
       " 'theater': 470,\n",
       " 'sequel': 471,\n",
       " 'ago': 472,\n",
       " 'result': 473,\n",
       " 'manage': 474,\n",
       " 'thriller': 475,\n",
       " 'catch': 476,\n",
       " 'spend': 477,\n",
       " 'sad': 478,\n",
       " 'english': 479,\n",
       " 'value': 480,\n",
       " 'female': 481,\n",
       " 'throw': 482,\n",
       " 'provide': 483,\n",
       " 'seriously': 484,\n",
       " 'scary': 485,\n",
       " 'dog': 486,\n",
       " 'lady': 487,\n",
       " 'none': 488,\n",
       " 'across': 489,\n",
       " 'alone': 490,\n",
       " 'check': 491,\n",
       " 'zombie': 492,\n",
       " 'spoiler': 493,\n",
       " 'possible': 494,\n",
       " 'exactly': 495,\n",
       " 'note': 496,\n",
       " 'team': 497,\n",
       " 'compare': 498,\n",
       " 'usually': 499,\n",
       " 'rock': 500,\n",
       " 'cinematography': 501,\n",
       " 'class': 502,\n",
       " 'strange': 503,\n",
       " 'huge': 504,\n",
       " 'comic': 505,\n",
       " 'subject': 506,\n",
       " 'dialog': 507,\n",
       " 'allow': 508,\n",
       " 'major': 509,\n",
       " 'whose': 510,\n",
       " 'offer': 511,\n",
       " 'explain': 512,\n",
       " 'middle': 513,\n",
       " 'producer': 514,\n",
       " 'deserve': 515,\n",
       " 'silly': 516,\n",
       " 'jack': 517,\n",
       " 'happy': 518,\n",
       " 'ridiculous': 519,\n",
       " 'local': 520,\n",
       " 'important': 521,\n",
       " 'cheap': 522,\n",
       " 'cover': 523,\n",
       " 'four': 524,\n",
       " 'message': 525,\n",
       " 'james': 526,\n",
       " 'win': 527,\n",
       " 'grow': 528,\n",
       " 'form': 529,\n",
       " 'easy': 530,\n",
       " 'mostly': 531,\n",
       " 'usual': 532,\n",
       " 'beyond': 533,\n",
       " 'tale': 534,\n",
       " 'season': 535,\n",
       " 'focus': 536,\n",
       " 'single': 537,\n",
       " 'somewhat': 538,\n",
       " 'attention': 539,\n",
       " 'produce': 540,\n",
       " 'parent': 541,\n",
       " 'shock': 542,\n",
       " 'pull': 543,\n",
       " 'amaze': 544,\n",
       " 'king': 545,\n",
       " 'drug': 546,\n",
       " 'apparently': 547,\n",
       " 'avoid': 548,\n",
       " 'non': 549,\n",
       " 'television': 550,\n",
       " 'modern': 551,\n",
       " 'upon': 552,\n",
       " 'due': 553,\n",
       " 'image': 554,\n",
       " 'street': 555,\n",
       " 'basically': 556,\n",
       " 'member': 557,\n",
       " 'earth': 558,\n",
       " 'space': 559,\n",
       " 'crime': 560,\n",
       " 'clichã': 561,\n",
       " 'fill': 562,\n",
       " 'george': 563,\n",
       " 'escape': 564,\n",
       " 'clearly': 565,\n",
       " 'near': 566,\n",
       " 'gun': 567,\n",
       " 'imagine': 568,\n",
       " 'fast': 569,\n",
       " 'boring': 570,\n",
       " 'oscar': 571,\n",
       " 'ten': 572,\n",
       " 'five': 573,\n",
       " 'hate': 574,\n",
       " 'fire': 575,\n",
       " 'air': 576,\n",
       " 'mystery': 577,\n",
       " 'french': 578,\n",
       " 'doubt': 579,\n",
       " 'deep': 580,\n",
       " 'filmmaker': 581,\n",
       " 'british': 582,\n",
       " 'charm': 583,\n",
       " 'entertainment': 584,\n",
       " 'clear': 585,\n",
       " 'straight': 586,\n",
       " 'build': 587,\n",
       " 'soundtrack': 588,\n",
       " 'predictable': 589,\n",
       " 'future': 590,\n",
       " 'weak': 591,\n",
       " 'remain': 592,\n",
       " 'fit': 593,\n",
       " 'whether': 594,\n",
       " 'ing': 595,\n",
       " 'feeling': 596,\n",
       " 'romantic': 597,\n",
       " 'ought': 598,\n",
       " 'detail': 599,\n",
       " 'stick': 600,\n",
       " 'easily': 601,\n",
       " 'peter': 602,\n",
       " 'enjoyable': 603,\n",
       " 'plan': 604,\n",
       " 'dull': 605,\n",
       " 'steal': 606,\n",
       " 'similar': 607,\n",
       " 'capture': 608,\n",
       " 'box': 609,\n",
       " 'mark': 610,\n",
       " 'soldier': 611,\n",
       " 'within': 612,\n",
       " 'doctor': 613,\n",
       " 'bunch': 614,\n",
       " 'western': 615,\n",
       " 'villain': 616,\n",
       " 'chase': 617,\n",
       " 'battle': 618,\n",
       " 'suspense': 619,\n",
       " 'emotion': 620,\n",
       " 'sorry': 621,\n",
       " 'carry': 622,\n",
       " 'storyline': 623,\n",
       " 'copy': 624,\n",
       " 'certain': 625,\n",
       " 'adult': 626,\n",
       " 'among': 627,\n",
       " 'aspect': 628,\n",
       " 'standard': 629,\n",
       " 'student': 630,\n",
       " 'period': 631,\n",
       " 'date': 632,\n",
       " 'water': 633,\n",
       " 'victim': 634,\n",
       " 'general': 635,\n",
       " 'attack': 636,\n",
       " 'dr': 637,\n",
       " 'typical': 638,\n",
       " 'de': 639,\n",
       " 'agree': 640,\n",
       " 'cartoon': 641,\n",
       " 'prove': 642,\n",
       " 'train': 643,\n",
       " 'material': 644,\n",
       " 'issue': 645,\n",
       " 'okay': 646,\n",
       " 'richard': 647,\n",
       " 'suck': 648,\n",
       " 'fantastic': 649,\n",
       " 'match': 650,\n",
       " 'realistic': 651,\n",
       " 'nearly': 652,\n",
       " 'wear': 653,\n",
       " 'mess': 654,\n",
       " 'red': 655,\n",
       " 'animation': 656,\n",
       " 'famous': 657,\n",
       " 'list': 658,\n",
       " 'paul': 659,\n",
       " 'draw': 660,\n",
       " 'romance': 661,\n",
       " 'alien': 662,\n",
       " 'large': 663,\n",
       " 'actual': 664,\n",
       " 'finish': 665,\n",
       " 'develop': 666,\n",
       " 'premise': 667,\n",
       " 'remind': 668,\n",
       " 'convince': 669,\n",
       " 'bill': 670,\n",
       " 'believable': 671,\n",
       " 'tom': 672,\n",
       " 'free': 673,\n",
       " 'hop': 674,\n",
       " 'somehow': 675,\n",
       " 'lame': 676,\n",
       " 'continue': 677,\n",
       " 'cry': 678,\n",
       " 'male': 679,\n",
       " 'masterpiece': 680,\n",
       " 'america': 681,\n",
       " 'average': 682,\n",
       " 'third': 683,\n",
       " 'truth': 684,\n",
       " 'sing': 685,\n",
       " 'background': 686,\n",
       " 'fly': 687,\n",
       " 'imdb': 688,\n",
       " 'party': 689,\n",
       " 'hot': 690,\n",
       " 'society': 691,\n",
       " 'york': 692,\n",
       " 'fi': 693,\n",
       " 'atmosphere': 694,\n",
       " 'baby': 695,\n",
       " 'bother': 696,\n",
       " 'stage': 697,\n",
       " 'gay': 698,\n",
       " 'eventually': 699,\n",
       " 'sci': 700,\n",
       " 'german': 701,\n",
       " 'whatever': 702,\n",
       " 'deliver': 703,\n",
       " 'particular': 704,\n",
       " 'crew': 705,\n",
       " 'indeed': 706,\n",
       " 'poorly': 707,\n",
       " 'vampire': 708,\n",
       " 'appreciate': 709,\n",
       " 'shame': 710,\n",
       " 'secret': 711,\n",
       " 'rate': 712,\n",
       " 'choice': 713,\n",
       " 'send': 714,\n",
       " 'forward': 715,\n",
       " 'spirit': 716,\n",
       " 'adventure': 717,\n",
       " 'fear': 718,\n",
       " 'accent': 719,\n",
       " 'week': 720,\n",
       " 'remake': 721,\n",
       " 'notice': 722,\n",
       " 'lee': 723,\n",
       " 'studio': 724,\n",
       " 'difficult': 725,\n",
       " 'admit': 726,\n",
       " 'appeal': 727,\n",
       " 'lie': 728,\n",
       " 'treat': 729,\n",
       " 'trouble': 730,\n",
       " 'beauty': 731,\n",
       " 'footage': 732,\n",
       " 'screenplay': 733,\n",
       " 'sexual': 734,\n",
       " 'emotional': 735,\n",
       " 'unless': 736,\n",
       " 'possibly': 737,\n",
       " 'wood': 738,\n",
       " 'island': 739,\n",
       " 'girlfriend': 740,\n",
       " 'memorable': 741,\n",
       " 'control': 742,\n",
       " 'dumb': 743,\n",
       " 'weird': 744,\n",
       " 'cheesy': 745,\n",
       " 'otherwise': 746,\n",
       " 'project': 747,\n",
       " 'nature': 748,\n",
       " 'award': 749,\n",
       " 'struggle': 750,\n",
       " 'plus': 751,\n",
       " 'crazy': 752,\n",
       " 'location': 753,\n",
       " 'lover': 754,\n",
       " 'animal': 755,\n",
       " 'describe': 756,\n",
       " 'superb': 757,\n",
       " 'bear': 758,\n",
       " 'fantasy': 759,\n",
       " 'rich': 760,\n",
       " 'choose': 761,\n",
       " 'beat': 762,\n",
       " 'development': 763,\n",
       " 'inside': 764,\n",
       " 'perfectly': 765,\n",
       " 'master': 766,\n",
       " 'discover': 767,\n",
       " 'badly': 768,\n",
       " 'rise': 769,\n",
       " 'previous': 770,\n",
       " 'japanese': 771,\n",
       " 'total': 772,\n",
       " 'personal': 773,\n",
       " 'blow': 774,\n",
       " 'maker': 775,\n",
       " 'quickly': 776,\n",
       " 'co': 777,\n",
       " 'mistake': 778,\n",
       " 'suggest': 779,\n",
       " 'costume': 780,\n",
       " 'front': 781,\n",
       " 'business': 782,\n",
       " 'disney': 783,\n",
       " 'respect': 784,\n",
       " 'rat': 785,\n",
       " 'towards': 786,\n",
       " 'creepy': 787,\n",
       " 'company': 788,\n",
       " 'interested': 789,\n",
       " 'joe': 790,\n",
       " 'incredibly': 791,\n",
       " 'term': 792,\n",
       " 'post': 793,\n",
       " 'land': 794,\n",
       " 'cat': 795,\n",
       " 'unique': 796,\n",
       " 'track': 797,\n",
       " 'amount': 798,\n",
       " 'hat': 799,\n",
       " 'powerful': 800,\n",
       " 'dramatic': 801,\n",
       " 'confuse': 802,\n",
       " 'scream': 803,\n",
       " 'roll': 804,\n",
       " 'color': 805,\n",
       " 'plenty': 806,\n",
       " 'band': 807,\n",
       " 'eak': 808,\n",
       " 'various': 809,\n",
       " 'answer': 810,\n",
       " 'store': 811,\n",
       " 'era': 812,\n",
       " 'dress': 813,\n",
       " 'marry': 814,\n",
       " 'success': 815,\n",
       " 'ings': 816,\n",
       " 'flaw': 817,\n",
       " 'warn': 818,\n",
       " 'century': 819,\n",
       " 'political': 820,\n",
       " 'apart': 821,\n",
       " 'blue': 822,\n",
       " 'law': 823,\n",
       " 'italian': 824,\n",
       " 'fairly': 825,\n",
       " 'memory': 826,\n",
       " 'channel': 827,\n",
       " 'outside': 828,\n",
       " 'pass': 829,\n",
       " 'william': 830,\n",
       " 'teen': 831,\n",
       " 'plain': 832,\n",
       " 'jump': 833,\n",
       " 'portrayal': 834,\n",
       " 'la': 835,\n",
       " 'kick': 836,\n",
       " 'reveal': 837,\n",
       " 'eat': 838,\n",
       " 'rip': 839,\n",
       " 'office': 840,\n",
       " 'odd': 841,\n",
       " 'creature': 842,\n",
       " 'hole': 843,\n",
       " 'player': 844,\n",
       " 'disappointed': 845,\n",
       " 'concept': 846,\n",
       " 'cute': 847,\n",
       " 'contain': 848,\n",
       " 'design': 849,\n",
       " 'ability': 850,\n",
       " 'heard': 851,\n",
       " 'recently': 852,\n",
       " 'ghost': 853,\n",
       " 'talented': 854,\n",
       " 'unlike': 855,\n",
       " 'clever': 856,\n",
       " 'hardly': 857,\n",
       " 'appearance': 858,\n",
       " 'flat': 859,\n",
       " 'culture': 860,\n",
       " 'door': 861,\n",
       " 'language': 862,\n",
       " 'step': 863,\n",
       " 'share': 864,\n",
       " 'nudity': 865,\n",
       " 'public': 866,\n",
       " 'camp': 867,\n",
       " 'tension': 868,\n",
       " 'pure': 869,\n",
       " 'sell': 870,\n",
       " 'listen': 871,\n",
       " 'potential': 872,\n",
       " 'travel': 873,\n",
       " 'disappoint': 874,\n",
       " 'depth': 875,\n",
       " 'spot': 876,\n",
       " 'excite': 877,\n",
       " 'sweet': 878,\n",
       " 'exist': 879,\n",
       " 'sleep': 880,\n",
       " 'record': 881,\n",
       " 'cold': 882,\n",
       " 'van': 883,\n",
       " 'fake': 884,\n",
       " 'scott': 885,\n",
       " 'science': 886,\n",
       " 'claim': 887,\n",
       " 'incredible': 888,\n",
       " 'sadly': 889,\n",
       " 'reach': 890,\n",
       " 'visual': 891,\n",
       " 'search': 892,\n",
       " 'familiar': 893,\n",
       " 'slightly': 894,\n",
       " 'pop': 895,\n",
       " 'suffer': 896,\n",
       " 'taste': 897,\n",
       " 'worst': 898,\n",
       " 'tone': 899,\n",
       " 'million': 900,\n",
       " 'race': 901,\n",
       " 'park': 902,\n",
       " 'gang': 903,\n",
       " 'hair': 904,\n",
       " 'popular': 905,\n",
       " 'purpose': 906,\n",
       " 'bar': 907,\n",
       " 'accept': 908,\n",
       " 'trash': 909,\n",
       " 'anti': 910,\n",
       " 'indian': 911,\n",
       " 'promise': 912,\n",
       " 'college': 913,\n",
       " 'approach': 914,\n",
       " 'trip': 915,\n",
       " 'social': 916,\n",
       " 'cross': 917,\n",
       " 'jane': 918,\n",
       " 'amazing': 919,\n",
       " 'ruin': 920,\n",
       " 'entirely': 921,\n",
       " 'amuse': 922,\n",
       " 'critic': 923,\n",
       " 'neither': 924,\n",
       " 'intelligent': 925,\n",
       " 'soul': 926,\n",
       " 'former': 927,\n",
       " 'common': 928,\n",
       " 'mad': 929,\n",
       " 'basic': 930,\n",
       " 'tear': 931,\n",
       " 'suddenly': 932,\n",
       " 'edge': 933,\n",
       " 'positive': 934,\n",
       " 'drop': 935,\n",
       " 'alive': 936,\n",
       " 'suit': 937,\n",
       " 'violent': 938,\n",
       " 'serve': 939,\n",
       " 'drag': 940,\n",
       " 'genius': 941,\n",
       " 'hurt': 942,\n",
       " 'disturb': 943,\n",
       " 'yeah': 944,\n",
       " 'adaptation': 945,\n",
       " 'computer': 946,\n",
       " 'ship': 947,\n",
       " 'ring': 948,\n",
       " 'perform': 949,\n",
       " 'slasher': 950,\n",
       " 'fiction': 951,\n",
       " 'destroy': 952,\n",
       " 'sick': 953,\n",
       " 'count': 954,\n",
       " 'judge': 955,\n",
       " 'successful': 956,\n",
       " 'christmas': 957,\n",
       " 'conclusion': 958,\n",
       " 'road': 959,\n",
       " 'pointless': 960,\n",
       " 'suspect': 961,\n",
       " 'fashion': 962,\n",
       " 'becomes': 963,\n",
       " 'pathetic': 964,\n",
       " 'burn': 965,\n",
       " 'aside': 966,\n",
       " 'ride': 967,\n",
       " 'bizarre': 968,\n",
       " 'bond': 969,\n",
       " 'prison': 970,\n",
       " 'teenager': 971,\n",
       " 'survive': 972,\n",
       " 'earlier': 973,\n",
       " 'scientist': 974,\n",
       " 'revenge': 975,\n",
       " 'ben': 976,\n",
       " 'grade': 977,\n",
       " 'ultimately': 978,\n",
       " 'raise': 979,\n",
       " 'foot': 980,\n",
       " 'handle': 981,\n",
       " 'honest': 982,\n",
       " 'effective': 983,\n",
       " 'recent': 984,\n",
       " 'awesome': 985,\n",
       " 'tough': 986,\n",
       " 'super': 987,\n",
       " 'extra': 988,\n",
       " 'mood': 989,\n",
       " 'agent': 990,\n",
       " 'ground': 991,\n",
       " 'garbage': 992,\n",
       " 'humour': 993,\n",
       " 'barely': 994,\n",
       " 'army': 995,\n",
       " 'hide': 996,\n",
       " 'motion': 997,\n",
       " 'reference': 998,\n",
       " 'impressive': 999,\n",
       " 'impossible': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_instance_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convertSentToVect(txtInput):\n",
    "    inputTxt = preProcess(replace_contraction(txtInput))    \n",
    "    # This is IMP KERAS text_to_word_sequence \n",
    "    inputTxt = text_to_word_sequence(POS_lemmatize(txtInput)) \n",
    "    #print(inputTxt)\n",
    "    \n",
    "    word_to_int= prod_instance_tokenizer.texts_to_sequences(inputTxt) # mapping it to int value from vocab    \n",
    "    seq =  [sum(word_to_int,[])] \n",
    "    #print(seq)\n",
    "    #ONLY PADDING IS REMAINIG  \n",
    "    padded = sequence.pad_sequences(seq , maxlen=INPUT_TEXT_LENGTH,padding='pre', truncating='pre')  \n",
    "    return padded\n",
    "                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'txtInput = \"They didn’t fit either. Straight high sticks at the end. On par with other buds I have. Lesson learned to avoid.\"\\nval = preProcessUserInput(txtInput)'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''txtInput = \"They didn’t fit either. Straight high sticks at the end. On par with other buds I have. Lesson learned to avoid.\"\n",
    "val = preProcessUserInput(txtInput)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 294)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Contruct model and weghits for prediction                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#used only if we export seperatly\n",
    "def init(): \n",
    "\tjson_file = open(MODEL_FILEPATH,'r')\n",
    "\tloaded_model_json = json_file.read()\n",
    "\tjson_file.close()\n",
    "\tloaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    \n",
    "\t#load woeights into new model\n",
    "\tloaded_model.load_weights(WEIGHT_FILEPATH)\n",
    "\tprint(\"Loaded Weight from disk\")\n",
    "\n",
    "\t#compile and evaluate loaded model\n",
    "\tloaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\t#loss,accuracy = model.evaluate(X_test,y_test)\n",
    "\t#print('loss:', loss)\n",
    "\t#print('accuracy:', accuracy)\n",
    "\tgraph = tf.get_default_graph()\n",
    "\n",
    "\treturn loaded_model,graph\n",
    "'''\n",
    "#used this coz we used Model Checkpointing\n",
    "from keras.models import load_model\n",
    "new_model = load_model(MODEL_FILEPATH)\n",
    "## On web it expects beow line\n",
    "## new_model._make_predict_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_prediction(txtInput):\n",
    "        val = convertSentToVect(txtInput)\n",
    "        \n",
    "        #sentiment = new_model.predict_classes(val)\n",
    "        sentiment = new_model.predict_proba(val)\n",
    "        print(sentiment)\n",
    "        '''\n",
    "        if sentiment[0] == 1:\n",
    "            print(\"positive\")\n",
    "           \n",
    "        elif  sentiment[0] == 0:  \n",
    "            print(\"negative\")\n",
    "          \n",
    "        else:\n",
    "            print('ss',sentiment)\n",
    "        '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9157539]]\n",
      "[[0.65572906]]\n",
      "[[0.48763373]]\n",
      "[[0.58145475]]\n",
      "[[0.51110476]]\n",
      "[[0.10963217]]\n",
      "[[0.65572906]]\n",
      "[[0.49974877]]\n",
      "[[0.08980442]]\n",
      "[[0.47798303]]\n",
      "[[0.03558333]]\n"
     ]
    }
   ],
   "source": [
    "# 1 is pos \n",
    "\n",
    "text1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
    "text2 = \"Good movie!\"\n",
    "text3 = \"Maybe I like this movie.\"\n",
    "text4 = \"Meh ...\"\n",
    "text5 = \"If I were a drunk teenager then this movie might be good.\"\n",
    "text6 = \"Bad movie!\"\n",
    "text7 = \"Not a good movie!\"\n",
    "text8 = \"This movie really sucks! Can I get my money back please?\"\n",
    "text9 = \"No scenario, bad actors Give a such budget to make this... In Belgium, we make ten films which win all prices in Cannes with this.<br /><br />Last time that I've seen a such NULL-Film was Hypercube. But scenario was better.<br /><br />Is anyone knows if the director was a graduate in school-film or a cop ?<br /><br />The better things in this film was the word Why authorize to sell this ? 1Ã§ is to expensive. <br /><br />I've pay ten dollars to buy this.<br /><br />For me, pay for this was my BIG MISTAKE of millennium.<br /><br />Too bad.<br /><br />Next time I'll break my arm but buy this type of sh*t\"\n",
    "text10 = \"Wealthy horse ranchers in Buenos Aires have a long-standing no-trading policy with the Crawfords of Manhattan, but what happens when the mustachioed Latin son falls for a certain Crawford with bright eyes, blonde hair, and some perky moves on the dance floor? 20th Century-Fox musical has a glossy veneer yet seems a bit tatty around the edges. It is very heavy on the frenetic, gymnastic-like dancing, exceedingly thin on story. Betty Grable (an eleventh hour replacement for Alice Faye) gives it a boost, even though she's paired with leaden Don Ameche (in tan make-up and slick hair). Also good: Charlotte Greenwood as Betty's pithy aunt, a limousine driver who's constantly asleep on the job, and Carmen Miranda playing herself (who else?). The stock shots of Argentina far outclass the action filmed on the Fox backlot, and some of the supporting performances are quite awful. By the time of the big horserace finale, most viewers will have had enough.\"\n",
    "text11 = \"The package received was blank and has no barcode. A waste of time and money.\"\n",
    "texts = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10, text11]\n",
    "\n",
    "for i, val in enumerate(texts):\n",
    "    perform_prediction(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = prod_instance_tokenizer.texts_to_sequences(text_to_word_sequence(\"movie ups downs good stuff movie much outweighs bad good movie indeed sometimes dialogue sound light one noticed way set light amateur act good highly original storyline intense atmosphere gore factor high effect do supremely definitely worth watch maybe even must see horror gore fan\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1516, 8, 392, 1, 24, 17, 8, 1, 706, 414, 294, 186, 313, 3, 4703, 34, 89, 313, 1865, 36, 8, 423, 117, 623, 1311, 694, 461, 1567, 168, 139, 125, 7979, 312, 183, 14, 179, 15, 111, 6, 90, 461, 106]\n"
     ]
    }
   ],
   "source": [
    "print(sum(seq,[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
