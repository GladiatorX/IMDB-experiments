{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refer /Keras /1. preProcess with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@jatinmandav3/opinion-mining-sometimes-known-as-sentiment-analysis-or-emotion-ai-refers-to-the-use-of-natural-874f369194c0\n",
    "#here abv link he has trained fastext model seperatly on all data\n",
    "\n",
    "#and used this as embedding data\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU,LSTM, Embedding,Flatten, Dropout,CuDNNGRU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "from gensim.models.wrappers import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "PROJ_NAME= \"2.1 Fastext_pretrain_300dims_20k_drop_LSTM_train\"\n",
    "\n",
    "MODEL_FILEPATH = f'model_tokenizers_weights_and_json/{PROJ_NAME}_best_model.h5'\n",
    "#WEIGHT_FILEPATH = f'model_tokenizers_weights_and_json/{PROJ_NAME}_best_weights.h5'\n",
    "IMAGE_PATH = f'model_images/{PROJ_NAME}.png'\n",
    "TOKENIZER_FILEPATH = f'model_tokenizers_weights_and_json/{PROJ_NAME}_tokenizer_instance.pickle'\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=f'logs/{PROJ_NAME}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making it reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "\n",
    "np.random.seed(seed)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEXT_LENGTH = 294 # from previous notebook\n",
    "VOCAB_SIZE = 20000      # vocab size needs to be taken care off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = pd.read_pickle(\"./pickles/V1_preProcessed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check if 1st 50k are shuffuled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>movie nothing like book think writer screenpla...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>first watched flatliners amaze necessary featu...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>see movie find hard understand many people see...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>work class romantic drama director martin ritt...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>admit great majority film release say dozen ma...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50001</th>\n",
       "      <td>take low budget inexperienced actor double pro...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment  \\\n",
       "49996  movie nothing like book think writer screenpla...        0.0   \n",
       "49997  first watched flatliners amaze necessary featu...        1.0   \n",
       "49998  see movie find hard understand many people see...        1.0   \n",
       "49999  work class romantic drama director martin ritt...        1.0   \n",
       "50000  admit great majority film release say dozen ma...        2.0   \n",
       "50001  take low budget inexperienced actor double pro...        2.0   \n",
       "\n",
       "       word_count  \n",
       "49996          55  \n",
       "49997          89  \n",
       "49998          87  \n",
       "49999          92  \n",
       "50000          73  \n",
       "50001          88  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed[49996:50002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# consider only 1st 50 for train and test split it from restÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = df_preprocessed[0:49999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    25000\n",
       "1.0    24999\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorising usin Keras\n",
    "- Setting integer value to a string token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# create the tokenizer\n",
    "tokenizer =Tokenizer(num_words=VOCAB_SIZE)\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(df[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = tokenizer.word_index\n",
    "#print(total_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up vocab size for FASTEXT handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "SELECT TOP 20K dict\n",
    "'''\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return dict(islice(iterable, n))\n",
    "\n",
    "vocab_20k = take(VOCAB_SIZE, total_vocab.items())\n",
    "\n",
    "\n",
    "type(vocab_20k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_20k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vocab_20k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text data to abv to integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seq = tokenizer.texts_to_sequences(df[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie ups downs good stuff movie much outweighs bad good movie indeed sometimes dialogue sound light one noticed way set light amateur act good highly original storyline intense atmosphere gore factor high effect do supremely definitely worth watch maybe even must see horror gore fan'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df['review'][155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1, 1516,    8,  392,    1,   24,   17,    8,    1,  706,  414,\n",
       "        294,  186,  313,    3, 4703,   34,   89,  313, 1865,   36,    8,\n",
       "        423,  117,  623, 1311,  694,  461, 1567,  168,  139,  125, 7979,\n",
       "        312,  183,   14,  179,   15,  111,    6,   90,  461,  106])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(seq[155])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "WE CAN SEE THE DIFFERENCE 10 , BCOZ OTHER VALUES ARE GRATER THEN VOCAB SIZE 20000 ; SO ITS NOT CONSIDERED\n",
    "'''\n",
    "print(df['word_count'][155])\n",
    "print(len(np.array(seq[155])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Actual VOCAB_SIZE deined:  20000\n",
      " Actual tokens created:  91671\n"
     ]
    }
   ],
   "source": [
    "print(\" Actual VOCAB_SIZE deined: \",VOCAB_SIZE)\n",
    "print(\" Actual tokens created: \",len(tokenizer.word_index))# IF WE WANT TO CONSIDER EVERY TEXT IN VOCAB PASS THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding Vocabulary for EMB. matrix\n",
    "* we can load either 100dims, 300dims * VOCAB_SIZE_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing pre trained FASTEXT\n",
    "* https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "                                                                #100d\n",
    "f = 'D:/dataset/Embedding/Fastext/wiki-news-300d-1M.vec'#encoding=\"utf8\")\n",
    "\n",
    "FASTEXT_Embedding = KeyedVectors.load_word2vec_format(f)#dict()\n",
    "\n",
    "'''\n",
    "for line in f:\n",
    "\tvalues = line.split() # split by lines\n",
    "\tword = values[0]# get 1st word in the line i.e the \"word\" itself\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32') # the preceding vector is the vector itself\n",
    "\tGlove_Embedding[word] = coefs# its a dict of word(key) and vector(value)\n",
    "f.close() \n",
    "\n",
    "print('Loaded %s word vectors.' % len(FASTEXT_Embedding))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.Word2VecKeyedVectors'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8164522051811218),\n",
       " ('guy', 0.7215225100517273),\n",
       " ('men', 0.7040212154388428),\n",
       " ('person', 0.6869154572486877),\n",
       " ('boy', 0.6806781888008118),\n",
       " ('Man', 0.6714824438095093),\n",
       " ('lady', 0.6661176681518555),\n",
       " ('girl', 0.6615854501724243),\n",
       " ('gentleman', 0.6586242318153381),\n",
       " ('dude', 0.6518971920013428)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(FASTEXT_Embedding))\n",
    "FASTEXT_Embedding.most_similar(positive=['man'],topn=10,restrict_vocab=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre trained FASTEXT, used as initilzation for our task\n",
    "* we create 2D array of size (VOCAB_SIZE * 300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#computer' in fb_model.wv.vocab\n",
    "'''\n",
    "https://radimrehurek.com/gensim/models/fasttext.html\n",
    "Retrieve word-vector for vocab and out-of-vocab word:\n",
    "You can perform various NLP word tasks with the model, some of them are already built-in:\n",
    "'''\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE+1,300))\n",
    "##words_not_found = []   # words not found in Glove emb. w.r.t our data corpous\n",
    "import collections\n",
    "words_not_found = collections.defaultdict(list)\n",
    "\n",
    "for word, i in vocab_20k.items():\n",
    "    if word in FASTEXT_Embedding.wv.vocab:\n",
    "        FASTEXT_vector_reprsentation = FASTEXT_Embedding[word]#Glove_Embedding.get(word,\"NULL\")\n",
    "    else:\n",
    "        ##words_not_found.append(word)\n",
    "        words_not_found[word].append(i)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'illiant': [426],\n",
       "             'clichÃ£': [561],\n",
       "             'ief': [1212],\n",
       "             'eaking': [1359],\n",
       "             'utal': [1526],\n",
       "             'oken': [1724],\n",
       "             'inging': [1745],\n",
       "             'eaks': [1755],\n",
       "             'illiantly': [1850],\n",
       "             'seagal': [1874],\n",
       "             'fulci': [2236],\n",
       "             'columbo': [2415],\n",
       "             'pacino': [2537],\n",
       "             'stanwyck': [2569],\n",
       "             'eathtaking': [2674],\n",
       "             'welles': [2677],\n",
       "             'matthau': [2780],\n",
       "             'iefly': [2812],\n",
       "             'lugosi': [2825],\n",
       "             'fiancÃ£': [2838],\n",
       "             'idge': [2926],\n",
       "             'austen': [2989],\n",
       "             'illiance': [3050],\n",
       "             'carrey': [3125],\n",
       "             'walken': [3134],\n",
       "             'oadcast': [3243],\n",
       "             'cusack': [3262],\n",
       "             'branagh': [3328],\n",
       "             'damme': [3415],\n",
       "             'troma': [3522],\n",
       "             'custer': [3562],\n",
       "             'cassavetes': [3563],\n",
       "             'cagney': [3565],\n",
       "             'karloff': [3599],\n",
       "             'lemmon': [3662],\n",
       "             'utally': [3692],\n",
       "             'miyazaki': [3718],\n",
       "             'huston': [3735],\n",
       "             'carradine': [3778],\n",
       "             'corman': [3866],\n",
       "             'meryl': [3954],\n",
       "             'quaid': [4000],\n",
       "             'widmark': [4062],\n",
       "             'voight': [4149],\n",
       "             'bettie': [4151],\n",
       "             'barrymore': [4231],\n",
       "             'miike': [4241],\n",
       "             'elvira': [4243],\n",
       "             'sarandon': [4258],\n",
       "             'astaire': [4367],\n",
       "             'belushi': [4369],\n",
       "             'rourke': [4391],\n",
       "             'mildred': [4523],\n",
       "             'utality': [4548],\n",
       "             'travolta': [4575],\n",
       "             'brosnan': [4603],\n",
       "             'mitchum': [4664],\n",
       "             'orson': [4701],\n",
       "             'busey': [4715],\n",
       "             'coppola': [4726],\n",
       "             'hayworth': [4774],\n",
       "             'errol': [4799],\n",
       "             'paxton': [4805],\n",
       "             'carell': [4863],\n",
       "             'duvall': [4882],\n",
       "             'verhoeven': [4920],\n",
       "             'deniro': [4921],\n",
       "             'devito': [4996],\n",
       "             'pfeiffer': [5017],\n",
       "             'shatner': [5028],\n",
       "             'uptly': [5059],\n",
       "             'itÃ¢': [5075],\n",
       "             'nolte': [5097],\n",
       "             'antonioni': [5126],\n",
       "             'toole': [5133],\n",
       "             'scorsese': [5147],\n",
       "             'eisenstein': [5168],\n",
       "             'preminger': [5196],\n",
       "             'visconti': [5213],\n",
       "             'rhett': [5304],\n",
       "             'swayze': [5318],\n",
       "             'palance': [5334],\n",
       "             'eakdown': [5340],\n",
       "             'kurosawa': [5357],\n",
       "             'ooding': [5401],\n",
       "             'basinger': [5445],\n",
       "             'depardieu': [5453],\n",
       "             'juliette': [5466],\n",
       "             'ainless': [5478],\n",
       "             'ustinov': [5522],\n",
       "             'redford': [5523],\n",
       "             'dolph': [5550],\n",
       "             'madsen': [5569],\n",
       "             'lundgren': [5619],\n",
       "             'knightley': [5667],\n",
       "             'sirk': [5672],\n",
       "             'eathing': [5673],\n",
       "             'lucille': [5698],\n",
       "             'arquette': [5730],\n",
       "             'lumet': [5735],\n",
       "             'monkees': [5741],\n",
       "             'kinnear': [5785],\n",
       "             'kilmer': [5807],\n",
       "             'bacall': [5810],\n",
       "             'soderbergh': [5827],\n",
       "             'lupino': [5846],\n",
       "             'kinski': [5856],\n",
       "             'hawn': [5886],\n",
       "             'niven': [5899],\n",
       "             'malkovich': [5911],\n",
       "             'lansbury': [5920],\n",
       "             'costner': [5922],\n",
       "             'excruciate': [5936],\n",
       "             'cheadle': [5944],\n",
       "             'lucio': [6004],\n",
       "             'cronenberg': [6010],\n",
       "             'grier': [6062],\n",
       "             'glenda': [6068],\n",
       "             'shemp': [6105],\n",
       "             'timon': [6114],\n",
       "             'gein': [6143],\n",
       "             'fiennes': [6151],\n",
       "             'gamera': [6166],\n",
       "             'attenborough': [6171],\n",
       "             'iÃ¢': [6192],\n",
       "             'bunuel': [6210],\n",
       "             'kristofferson': [6214],\n",
       "             'tierney': [6218],\n",
       "             'hackman': [6231],\n",
       "             'mclaglen': [6276],\n",
       "             'lestat': [6290],\n",
       "             'fassbinder': [6310],\n",
       "             'fishburne': [6314],\n",
       "             'antwone': [6335],\n",
       "             'biko': [6336],\n",
       "             'godard': [6338],\n",
       "             'wayans': [6339],\n",
       "             'myrna': [6356],\n",
       "             'dietrich': [6359],\n",
       "             'poirot': [6360],\n",
       "             'finney': [6375],\n",
       "             'lorre': [6380],\n",
       "             'redgrave': [6395],\n",
       "             'pickford': [6401],\n",
       "             'fellini': [6402],\n",
       "             'keanu': [6412],\n",
       "             'chupaca': [6414],\n",
       "             'naschy': [6415],\n",
       "             'zellweger': [6432],\n",
       "             'rathbone': [6462],\n",
       "             'dahmer': [6481],\n",
       "             'huppert': [6484],\n",
       "             'moriarty': [6522],\n",
       "             'gershwin': [6539],\n",
       "             'brynner': [6546],\n",
       "             'gulliver': [6580],\n",
       "             'bava': [6586],\n",
       "             'babette': [6619],\n",
       "             'deneuve': [6628],\n",
       "             'demille': [6637],\n",
       "             'thurman': [6659],\n",
       "             'caligula': [6669],\n",
       "             'gielgud': [6677],\n",
       "             'vincenzo': [6689],\n",
       "             'deanna': [6694],\n",
       "             'eathe': [6697],\n",
       "             'lubitsch': [6784],\n",
       "             'kolchak': [6792],\n",
       "             'gÃ£': [6800],\n",
       "             'impend': [6802],\n",
       "             'rko': [6830],\n",
       "             'marlow': [6869],\n",
       "             'dafoe': [6871],\n",
       "             'selleck': [6889],\n",
       "             'rickman': [6918],\n",
       "             'martino': [6925],\n",
       "             'milland': [6932],\n",
       "             'poitier': [6949],\n",
       "             'fairbanks': [6972],\n",
       "             'masterson': [6982],\n",
       "             'estevez': [6987],\n",
       "             'duchovny': [6990],\n",
       "             'ratso': [6997],\n",
       "             'minnelli': [7003],\n",
       "             'wynorski': [7057],\n",
       "             'farscape': [7060],\n",
       "             'rowlands': [7063],\n",
       "             'scheider': [7069],\n",
       "             'veronika': [7070],\n",
       "             'arkin': [7076],\n",
       "             'sydow': [7110],\n",
       "             'atwill': [7115],\n",
       "             'gilliam': [7128],\n",
       "             'midler': [7155],\n",
       "             'eakfast': [7176],\n",
       "             'quigley': [7193],\n",
       "             'dolemite': [7197],\n",
       "             'coen': [7202],\n",
       "             'pegg': [7203],\n",
       "             'seuss': [7218],\n",
       "             'raimi': [7245],\n",
       "             'sammo': [7270],\n",
       "             'durbin': [7294],\n",
       "             'krueger': [7311],\n",
       "             'mathieu': [7358],\n",
       "             'dassin': [7425],\n",
       "             'dressler': [7426],\n",
       "             'geraldine': [7439],\n",
       "             'dÃ£': [7468],\n",
       "             'thelma': [7482],\n",
       "             'ironside': [7492],\n",
       "             'bynes': [7512],\n",
       "             'bandw': [7514],\n",
       "             'crispin': [7516],\n",
       "             'unette': [7525],\n",
       "             'truffaut': [7539],\n",
       "             'colman': [7617],\n",
       "             'kutcher': [7637],\n",
       "             'mulholland': [7663],\n",
       "             'raoul': [7671],\n",
       "             'carlito': [7685],\n",
       "             'gunga': [7694],\n",
       "             'goldsworthy': [7695],\n",
       "             'capshaw': [7696],\n",
       "             'schrader': [7697],\n",
       "             'eakthrough': [7749],\n",
       "             'katharine': [7767],\n",
       "             'sosuke': [7775],\n",
       "             'dreyfus': [7790],\n",
       "             'rockne': [7792],\n",
       "             'bsg': [7799],\n",
       "             'keitel': [7847],\n",
       "             'prue': [7852],\n",
       "             'marisa': [7880],\n",
       "             'kieslowski': [7918],\n",
       "             'langdon': [7942],\n",
       "             'brigitte': [7957],\n",
       "             'cassel': [7962],\n",
       "             'liotta': [7964],\n",
       "             'milligan': [7965],\n",
       "             'northam': [7996],\n",
       "             'melvyn': [8011],\n",
       "             'marlene': [8012],\n",
       "             'sonja': [8040],\n",
       "             'binoche': [8046],\n",
       "             'englund': [8049],\n",
       "             'mishima': [8050],\n",
       "             'rififi': [8055],\n",
       "             'fleischer': [8089],\n",
       "             'delon': [8107],\n",
       "             'cimino': [8111],\n",
       "             'harlin': [8115],\n",
       "             'zucco': [8126],\n",
       "             'bardem': [8137],\n",
       "             'nevsky': [8150],\n",
       "             'tomei': [8158],\n",
       "             'demme': [8175],\n",
       "             'gypo': [8202],\n",
       "             'laputa': [8206],\n",
       "             'blondell': [8220],\n",
       "             'renoir': [8222],\n",
       "             'kiefer': [8224],\n",
       "             'paresh': [8227],\n",
       "             'eccleston': [8232],\n",
       "             'potemkin': [8241],\n",
       "             'beckinsale': [8256],\n",
       "             'bonham': [8259],\n",
       "             'flavia': [8275],\n",
       "             'dillinger': [8302],\n",
       "             'dangerfield': [8311],\n",
       "             'takashi': [8314],\n",
       "             'gyllenhaal': [8325],\n",
       "             'rohmer': [8329],\n",
       "             'emmanuelle': [8343],\n",
       "             'maltin': [8368],\n",
       "             'cheech': [8381],\n",
       "             'turturro': [8401],\n",
       "             'pumbaa': [8402],\n",
       "             'josÃ£': [8404],\n",
       "             'longoria': [8412],\n",
       "             'hamill': [8434],\n",
       "             'hasselhoff': [8446],\n",
       "             'greenstreet': [8452],\n",
       "             'collette': [8463],\n",
       "             'marjorie': [8519],\n",
       "             'suchet': [8531],\n",
       "             'perlman': [8539],\n",
       "             'bernsen': [8542],\n",
       "             'mamet': [8557],\n",
       "             'cafÃ£': [8566],\n",
       "             'oppenheimer': [8644],\n",
       "             'iturbi': [8647],\n",
       "             'stockwell': [8648],\n",
       "             'sondra': [8657],\n",
       "             'belmondo': [8670],\n",
       "             'schreiber': [8675],\n",
       "             'sampedro': [8678],\n",
       "             'eileen': [8683],\n",
       "             'linnea': [8684],\n",
       "             'minghella': [8693],\n",
       "             'evie': [8741],\n",
       "             'mattei': [8752],\n",
       "             'willem': [8758],\n",
       "             'lÃ£': [8773],\n",
       "             'forsythe': [8781],\n",
       "             'oldboy': [8782],\n",
       "             'pertwee': [8787],\n",
       "             'lamarr': [8812],\n",
       "             'mpaa': [8844],\n",
       "             'farnsworth': [8847],\n",
       "             'riddick': [8881],\n",
       "             'bogosian': [8891],\n",
       "             'caddyshack': [8892],\n",
       "             'leguizamo': [8895],\n",
       "             'savalas': [8903],\n",
       "             'henriksen': [8906],\n",
       "             'giovanna': [8908],\n",
       "             'roeg': [8956],\n",
       "             'greenaway': [8967],\n",
       "             'carax': [8973],\n",
       "             'julianne': [8977],\n",
       "             'hartnett': [8989],\n",
       "             'andie': [8991],\n",
       "             'bronte': [9004],\n",
       "             'creasy': [9016],\n",
       "             'mcgavin': [9036],\n",
       "             'cÃ£': [9039],\n",
       "             'peckinpah': [9058],\n",
       "             'sigourney': [9084],\n",
       "             'cinemax': [9095],\n",
       "             'blaise': [9109],\n",
       "             'hearn': [9112],\n",
       "             'ained': [9117],\n",
       "             'elisha': [9141],\n",
       "             'haruhi': [9176],\n",
       "             'harron': [9216],\n",
       "             'malden': [9220],\n",
       "             'levinson': [9222],\n",
       "             'gackt': [9256],\n",
       "             'armand': [9265],\n",
       "             'macmurray': [9268],\n",
       "             'dourif': [9269],\n",
       "             'matinÃ£': [9273],\n",
       "             'dicaprio': [9275],\n",
       "             'existenz': [9284],\n",
       "             'feinstone': [9297],\n",
       "             'cillian': [9300],\n",
       "             'rooker': [9301],\n",
       "             'ringwald': [9303],\n",
       "             'himesh': [9335],\n",
       "             'pixote': [9338],\n",
       "             'natali': [9339],\n",
       "             'gretchen': [9341],\n",
       "             'poppins': [9342],\n",
       "             'protÃ£': [9351],\n",
       "             'northfork': [9365],\n",
       "             'aschenbach': [9384],\n",
       "             'blandings': [9399],\n",
       "             'kells': [9407],\n",
       "             'sorvino': [9409],\n",
       "             'dudikoff': [9437],\n",
       "             'sholay': [9447],\n",
       "             'schindler': [9457],\n",
       "             'pleasance': [9469],\n",
       "             'ramÃ£Â³n': [9470],\n",
       "             'ichikawa': [9474],\n",
       "             'boorman': [9480],\n",
       "             'pauly': [9491],\n",
       "             'brashear': [9496],\n",
       "             'hedy': [9498],\n",
       "             'deol': [9504],\n",
       "             'neeson': [9511],\n",
       "             'modine': [9538],\n",
       "             'grint': [9549],\n",
       "             'astin': [9550],\n",
       "             'gilligan': [9557],\n",
       "             'flatliners': [9567],\n",
       "             'luzhin': [9590],\n",
       "             'aykroyd': [9612],\n",
       "             'almodovar': [9615],\n",
       "             'clouzot': [9616],\n",
       "             'claudius': [9624],\n",
       "             'kusturica': [9641],\n",
       "             'izzard': [9661],\n",
       "             'ingmar': [9679],\n",
       "             'ullman': [9680],\n",
       "             'caruso': [9699],\n",
       "             'mirren': [9702],\n",
       "             'buscemi': [9703],\n",
       "             'giamatti': [9705],\n",
       "             'danning': [9709],\n",
       "             'grahame': [9721],\n",
       "             'tamura': [9735],\n",
       "             'zucker': [9744],\n",
       "             'amityville': [9760],\n",
       "             'rÃ£': [9762],\n",
       "             'newhart': [9769],\n",
       "             'wentworth': [9789],\n",
       "             'crichton': [9793],\n",
       "             'savini': [9798],\n",
       "             'eezy': [9803],\n",
       "             'tashan': [9843],\n",
       "             'geena': [9846],\n",
       "             'epps': [9849],\n",
       "             'gandolfini': [9863],\n",
       "             'linney': [9866],\n",
       "             'caprica': [9867],\n",
       "             'heche': [9869],\n",
       "             'lithgow': [9874],\n",
       "             'dunst': [9885],\n",
       "             'sandm': [9900],\n",
       "             'morricone': [9902],\n",
       "             'mcintire': [9921],\n",
       "             'chÃ£': [9922],\n",
       "             'naÃ£': [9940],\n",
       "             'peebles': [9954],\n",
       "             'ringu': [9956],\n",
       "             'ribisi': [9958],\n",
       "             'bogdanovich': [9960],\n",
       "             'eathless': [9961],\n",
       "             'tarkovsky': [9962],\n",
       "             'dunaway': [9970],\n",
       "             'mcclure': [9980],\n",
       "             'gellar': [9981],\n",
       "             'cruella': [9986],\n",
       "             'hynkel': [10010],\n",
       "             'farrah': [10014],\n",
       "             'jÃ£': [10022],\n",
       "             'veidt': [10025],\n",
       "             'bruckheimer': [10041],\n",
       "             'aiello': [10045],\n",
       "             'ozu': [10054],\n",
       "             'underdevelop': [10056],\n",
       "             'womanize': [10070],\n",
       "             'thunderball': [10076],\n",
       "             'fujimori': [10085],\n",
       "             'leachman': [10089],\n",
       "             'mantegna': [10098],\n",
       "             'vidor': [10130],\n",
       "             'viggo': [10132],\n",
       "             'sammi': [10148],\n",
       "             'zelah': [10150],\n",
       "             'lumumba': [10155],\n",
       "             'statham': [10158],\n",
       "             'fagin': [10159],\n",
       "             'moreau': [10162],\n",
       "             'ainwashed': [10165],\n",
       "             'charlize': [10166],\n",
       "             'northanger': [10173],\n",
       "             'gleason': [10176],\n",
       "             'pavarotti': [10188],\n",
       "             'culkin': [10205],\n",
       "             'crenna': [10246],\n",
       "             'kareena': [10257],\n",
       "             'lindbergh': [10259],\n",
       "             'adrienne': [10268],\n",
       "             'curtiz': [10272],\n",
       "             'eeding': [10275],\n",
       "             'raveena': [10276],\n",
       "             'hanzo': [10278],\n",
       "             'panahi': [10282],\n",
       "             'depalma': [10294],\n",
       "             'shabana': [10295],\n",
       "             'danton': [10297],\n",
       "             'mastroianni': [10301],\n",
       "             'strangelove': [10318],\n",
       "             'dukakis': [10328],\n",
       "             'tsui': [10343],\n",
       "             'shawshank': [10345],\n",
       "             'mcadams': [10358],\n",
       "             'gwtw': [10361],\n",
       "             'havilland': [10380],\n",
       "             'harilal': [10402],\n",
       "             'verne': [10409],\n",
       "             'hitchcockian': [10425],\n",
       "             'mortensen': [10428],\n",
       "             'rizzo': [10430],\n",
       "             'loomis': [10431],\n",
       "             'swope': [10446],\n",
       "             'haneke': [10448],\n",
       "             'maugham': [10449],\n",
       "             'stoltz': [10450],\n",
       "             'se7en': [10456],\n",
       "             'kalifornia': [10458],\n",
       "             'crowhurst': [10460],\n",
       "             'jouvet': [10462],\n",
       "             'berenger': [10468],\n",
       "             'winslet': [10474],\n",
       "             'matheson': [10486],\n",
       "             'frederic': [10491],\n",
       "             'pÃ£': [10495],\n",
       "             'eleniak': [10501],\n",
       "             'othel': [10502],\n",
       "             'tromeo': [10505],\n",
       "             'wenders': [10526],\n",
       "             'yuzna': [10551],\n",
       "             'gossett': [10559],\n",
       "             'floriane': [10561],\n",
       "             'owsing': [10570],\n",
       "             'champlain': [10592],\n",
       "             'culp': [10599],\n",
       "             'dibiase': [10601],\n",
       "             'kronk': [10611],\n",
       "             'rosenstrasse': [10612],\n",
       "             'sherri': [10619],\n",
       "             'ossessione': [10625],\n",
       "             'azaria': [10626],\n",
       "             'spellbind': [10627],\n",
       "             'dogtown': [10653],\n",
       "             'tadzio': [10654],\n",
       "             'vittorio': [10663],\n",
       "             'andersson': [10668],\n",
       "             'foch': [10673],\n",
       "             'luthor': [10678],\n",
       "             'schlesinger': [10695],\n",
       "             'wallach': [10704],\n",
       "             'assy': [10718],\n",
       "             'eeze': [10723],\n",
       "             'deluise': [10730],\n",
       "             'carface': [10731],\n",
       "             'trancers': [10758],\n",
       "             'mostel': [10761],\n",
       "             'guttenberg': [10766],\n",
       "             'shue': [10767],\n",
       "             'pasolini': [10769],\n",
       "             'lillian': [10775],\n",
       "             'montand': [10782],\n",
       "             'kornbluth': [10789],\n",
       "             'koontz': [10805],\n",
       "             'courtenay': [10832],\n",
       "             'mahmut': [10864],\n",
       "             'joline': [10868],\n",
       "             'thatÃ¢': [10870],\n",
       "             'pyun': [10873],\n",
       "             'ightly': [10879],\n",
       "             'cleese': [10883],\n",
       "             'franÃ£': [10890],\n",
       "             'broderick': [10907],\n",
       "             'lommel': [10908],\n",
       "             'blethyn': [10913],\n",
       "             'deathstalker': [10914],\n",
       "             'ungar': [10916],\n",
       "             'juhi': [10929],\n",
       "             'roddy': [10949],\n",
       "             'kuzco': [10962],\n",
       "             'maclaine': [10966],\n",
       "             'kasparov': [10968],\n",
       "             'azumi': [10970],\n",
       "             'bobbie': [10994],\n",
       "             'ighter': [11011],\n",
       "             'onofrio': [11016],\n",
       "             'starewicz': [11056],\n",
       "             'grable': [11063],\n",
       "             'korda': [11079],\n",
       "             'cloris': [11081],\n",
       "             'malick': [11082],\n",
       "             'darryl': [11083],\n",
       "             'lovecraft': [11086],\n",
       "             'renny': [11088],\n",
       "             'labute': [11089],\n",
       "             'herbie': [11100],\n",
       "             'hoskins': [11115],\n",
       "             'hickock': [11118],\n",
       "             'waterston': [11127],\n",
       "             'keeler': [11146],\n",
       "             'amenabar': [11147],\n",
       "             'sarne': [11151],\n",
       "             'ameche': [11160],\n",
       "             'paget': [11161],\n",
       "             'dominick': [11162],\n",
       "             'rainer': [11174],\n",
       "             'icated': [11182],\n",
       "             'danson': [11205],\n",
       "             'duryea': [11212],\n",
       "             'steiger': [11213],\n",
       "             'vivien': [11243],\n",
       "             'askey': [11250],\n",
       "             'sematary': [11268],\n",
       "             'nazarin': [11300],\n",
       "             'aristocats': [11326],\n",
       "             'suspiria': [11333],\n",
       "             'crouse': [11335],\n",
       "             'inman': [11338],\n",
       "             'batwoman': [11348],\n",
       "             'khouri': [11350],\n",
       "             'milius': [11354],\n",
       "             'ennio': [11385],\n",
       "             'morita': [11401],\n",
       "             'spacek': [11411],\n",
       "             'iskie': [11416],\n",
       "             'chappelle': [11425],\n",
       "             'mcconaughey': [11440],\n",
       "             'radford': [11441],\n",
       "             'nimoy': [11463],\n",
       "             'sheffer': [11464],\n",
       "             'hardwicke': [11469],\n",
       "             'ghibli': [11470],\n",
       "             'uzumaki': [11475],\n",
       "             'blier': [11477],\n",
       "             'ronny': [11494],\n",
       "             'wray': [11496],\n",
       "             'macdowell': [11508],\n",
       "             'zatoichi': [11509],\n",
       "             'vasey': [11535],\n",
       "             'dorff': [11542],\n",
       "             'sondheim': [11557],\n",
       "             'cuthbert': [11560],\n",
       "             'lamberto': [11565],\n",
       "             'jayne': [11570],\n",
       "             'cukor': [11571],\n",
       "             'fawlty': [11580],\n",
       "             'eustache': [11582],\n",
       "             'emanuelle': [11584],\n",
       "             'robards': [11604],\n",
       "             'liev': [11620],\n",
       "             'hodder': [11622],\n",
       "             'mcnally': [11623],\n",
       "             'earp': [11625],\n",
       "             'janeane': [11626],\n",
       "             'stratten': [11636],\n",
       "             'darkman': [11647],\n",
       "             'margera': [11654],\n",
       "             'andrÃ£': [11659],\n",
       "             'icks': [11677],\n",
       "             'hayao': [11678],\n",
       "             'elwes': [11684],\n",
       "             'traci': [11685],\n",
       "             'sturges': [11698],\n",
       "             'buÃ£': [11712],\n",
       "             'keach': [11717],\n",
       "             'vivah': [11718],\n",
       "             'sinuhe': [11730],\n",
       "             'delpy': [11734],\n",
       "             'snider': [11738],\n",
       "             'paquin': [11748],\n",
       "             'btk': [11753],\n",
       "             'rosanna': [11759],\n",
       "             'putney': [11760],\n",
       "             'mcbain': [11774],\n",
       "             'kriemhild': [11777],\n",
       "             'coffy': [11778],\n",
       "             'rochon': [11780],\n",
       "             'baseketball': [11785],\n",
       "             'mayall': [11795],\n",
       "             'aande': [11802],\n",
       "             'elia': [11818],\n",
       "             'varney': [11821],\n",
       "             'florinda': [11829],\n",
       "             'bolkan': [11830],\n",
       "             'shakespearian': [11837],\n",
       "             'thinnes': [11839],\n",
       "             'coltrane': [11852],\n",
       "             'kellerman': [11872],\n",
       "             'leonora': [11873],\n",
       "             'carrell': [11890],\n",
       "             'mcshane': [11894],\n",
       "             'avely': [11899],\n",
       "             'titta': [11903],\n",
       "             'bellucci': [11917],\n",
       "             'risquÃ£': [11921],\n",
       "             'schaech': [11924],\n",
       "             'monaghan': [11931],\n",
       "             'weissmuller': [11936],\n",
       "             'susannah': [11938],\n",
       "             'rutger': [11945],\n",
       "             'newcombe': [11949],\n",
       "             'malley': [11950],\n",
       "             'arbuckle': [11951],\n",
       "             'serling': [11956],\n",
       "             'danner': [11957],\n",
       "             'wendt': [11964],\n",
       "             'eathtakingly': [11965],\n",
       "             'gigli': [11967],\n",
       "             'vincente': [11977],\n",
       "             'marsha': [11979],\n",
       "             'tigerland': [11983],\n",
       "             'knotts': [11990],\n",
       "             'tilney': [11996],\n",
       "             'bancroft': [12001],\n",
       "             'mahler': [12022],\n",
       "             'gazzara': [12025],\n",
       "             'aishwarya': [12027],\n",
       "             'fahey': [12052],\n",
       "             'avado': [12064],\n",
       "             'giancarlo': [12066],\n",
       "             'sullavan': [12069],\n",
       "             'doolittle': [12085],\n",
       "             'greystoke': [12086],\n",
       "             'romy': [12097],\n",
       "             'etooth': [12101],\n",
       "             'morty': [12105],\n",
       "             'trelkovsky': [12109],\n",
       "             'urmila': [12116],\n",
       "             'nilsson': [12122],\n",
       "             'frodo': [12130],\n",
       "             'jeffries': [12134],\n",
       "             'renÃ£': [12140],\n",
       "             'hauer': [12151],\n",
       "             'stepford': [12157],\n",
       "             'eaker': [12162],\n",
       "             'roddenberry': [12171],\n",
       "             'stahl': [12173],\n",
       "             'konkona': [12179],\n",
       "             'braff': [12180],\n",
       "             'dennehy': [12188],\n",
       "             'motorama': [12193],\n",
       "             'woronov': [12200],\n",
       "             'traffik': [12201],\n",
       "             'seberg': [12205],\n",
       "             'mithi': [12206],\n",
       "             'kharis': [12208],\n",
       "             'paperhouse': [12209],\n",
       "             'oader': [12216],\n",
       "             'jimmie': [12221],\n",
       "             'krige': [12243],\n",
       "             'merkerson': [12251],\n",
       "             'ackland': [12260],\n",
       "             'ightest': [12280],\n",
       "             'jaffar': [12287],\n",
       "             'canÃ¢': [12295],\n",
       "             'franchot': [12301],\n",
       "             'bluth': [12302],\n",
       "             'celie': [12309],\n",
       "             'sjÃ£': [12312],\n",
       "             'ranma': [12328],\n",
       "             'langella': [12332],\n",
       "             'digges': [12334],\n",
       "             'bettany': [12347],\n",
       "             'donnell': [12349],\n",
       "             'fricker': [12352],\n",
       "             'jaffe': [12358],\n",
       "             'drebin': [12359],\n",
       "             'brandauer': [12360],\n",
       "             'bukowski': [12365],\n",
       "             'magoo': [12373],\n",
       "             'avura': [12380],\n",
       "             'devgan': [12396],\n",
       "             'idges': [12397],\n",
       "             'renfro': [12408],\n",
       "             'hilliard': [12412],\n",
       "             'almasy': [12415],\n",
       "             'zenia': [12417],\n",
       "             'garson': [12424],\n",
       "             'flashdance': [12427],\n",
       "             'jannings': [12431],\n",
       "             'remem': [12462],\n",
       "             'irvin': [12463],\n",
       "             'lordi': [12473],\n",
       "             'iago': [12476],\n",
       "             'kristy': [12512],\n",
       "             'adjani': [12513],\n",
       "             'tautou': [12520],\n",
       "             'slausen': [12526],\n",
       "             'hackenstein': [12543],\n",
       "             'bathsheba': [12546],\n",
       "             'jeroen': [12551],\n",
       "             'krell': [12565],\n",
       "             'hussey': [12566],\n",
       "             'amelie': [12568],\n",
       "             'aardman': [12569],\n",
       "             'miramax': [12572],\n",
       "             'sheeta': [12587],\n",
       "             'bahrani': [12592],\n",
       "             'ushes': [12603],\n",
       "             'phillippe': [12606],\n",
       "             'barkin': [12610],\n",
       "             'pleasence': [12611],\n",
       "             'kirkland': [12612],\n",
       "             'maude': [12615],\n",
       "             'pazu': [12619],\n",
       "             'octopussy': [12624],\n",
       "             'benet': [12659],\n",
       "             'lorelai': [12662],\n",
       "             'daulton': [12671],\n",
       "             'mazursky': [12674],\n",
       "             'woodard': [12682],\n",
       "             'luise': [12733],\n",
       "             'luchino': [12735],\n",
       "             'oadcasting': [12752],\n",
       "             'labeouf': [12755],\n",
       "             'fineman': [12756],\n",
       "             'pinkett': [12757],\n",
       "             'esposito': [12762],\n",
       "             'deodato': [12765],\n",
       "             'brolin': [12776],\n",
       "             'scorcese': [12780],\n",
       "             'zanuck': [12782],\n",
       "             'rgv': [12785],\n",
       "             'cassavettes': [12786],\n",
       "             'eathes': [12807],\n",
       "             'tully': [12810],\n",
       "             'rajpal': [12818],\n",
       "             'royston': [12823],\n",
       "             'goldwyn': [12827],\n",
       "             'besson': [12832],\n",
       "             'vardon': [12859],\n",
       "             'breillat': [12861],\n",
       "             'pimlico': [12862],\n",
       "             'beals': [12871],\n",
       "             'beavis': [12876],\n",
       "             'mindy': [12908],\n",
       "             'eraserhead': [12926],\n",
       "             'burstyn': [12956],\n",
       "             'urich': [12978],\n",
       "             'cleef': [12979],\n",
       "             'hearst': [12986],\n",
       "             'baywatch': [12998],\n",
       "             'brimley': [13009],\n",
       "             'leelee': [13016],\n",
       "             'harrelson': [13022],\n",
       "             'patekar': [13030],\n",
       "             'ackroyd': [13033],\n",
       "             'clampett': [13034],\n",
       "             'yeon': [13040],\n",
       "             'ridgemont': [13045],\n",
       "             'creepshow': [13049],\n",
       "             'sayles': [13053],\n",
       "             'shepitko': [13057],\n",
       "             'footlight': [13065],\n",
       "             'nielson': [13067],\n",
       "             'nausicaa': [13070],\n",
       "             'bouchet': [13074],\n",
       "             'jonestown': [13075],\n",
       "             'tobel': [13076],\n",
       "             'wyler': [13078],\n",
       "             'mraovich': [13080],\n",
       "             'dmytryk': [13097],\n",
       "             'wodehouse': [13108],\n",
       "             'farrelly': [13109],\n",
       "             'hutz': [13121],\n",
       "             'sheedy': [13126],\n",
       "             'bogarde': [13134],\n",
       "             'brinke': [13137],\n",
       "             'robespierre': [13140],\n",
       "             'kazaam': [13141],\n",
       "             'grisham': [13143],\n",
       "             'morbius': [13144],\n",
       "             'romulan': [13145],\n",
       "             'garofalo': [13148],\n",
       "             'borgnine': [13150],\n",
       "             'heflin': [13183],\n",
       "             'alfre': [13200],\n",
       "             'kier': [13205],\n",
       "             'sternberg': [13208],\n",
       "             'railsback': [13210],\n",
       "             'aparna': [13219],\n",
       "             'raisuli': [13222],\n",
       "             'bardot': [13235],\n",
       "             'applegate': [13238],\n",
       "             'polarisdib': [13247],\n",
       "             'rhonda': [13258],\n",
       "             'mordrid': [13259],\n",
       "             'astÃ£': [13267],\n",
       "             'gwynne': [13271],\n",
       "             'devos': [13273],\n",
       "             'macgregor': [13279],\n",
       "             'roseanne': [13288],\n",
       "             'berkley': [13299],\n",
       "             'Ã¢Â½': [13306],\n",
       "             'asive': [13311],\n",
       "             'valeria': [13312],\n",
       "             'strÃ£': [13314],\n",
       "             'garlin': [13325],\n",
       "             'berkowitz': [13326],\n",
       "             'leatherface': [13329],\n",
       "             'sennett': [13332],\n",
       "             'knute': [13354],\n",
       "             'azen': [13359],\n",
       "             'kelso': [13367],\n",
       "             'yzma': [13369],\n",
       "             'tourneur': [13372],\n",
       "             'burman': [13374],\n",
       "             'gaiman': [13378],\n",
       "             'bÃ£': [13380],\n",
       "             'yimou': [13385],\n",
       "             'audiard': [13390],\n",
       "             'brackett': [13394],\n",
       "             'buttgereit': [13400],\n",
       "             'rockford': [13402],\n",
       "             'stapleton': [13403],\n",
       "             'edelman': [13404],\n",
       "             'hoppity': [13407],\n",
       "             'greengrass': [13409],\n",
       "             'imamura': [13421],\n",
       "             'mcgovern': [13423],\n",
       "             'lenzi': [13432],\n",
       "             'conrack': [13433],\n",
       "             'bullitt': [13435],\n",
       "             'mahoney': [13438],\n",
       "             'kattan': [13440],\n",
       "             'bassanio': [13443],\n",
       "             'gemser': [13466],\n",
       "             'ainer': [13469],\n",
       "             'mccool': [13516],\n",
       "             'macready': [13521],\n",
       "             'dandd': [13541],\n",
       "             'grieco': [13558],\n",
       "             'tamblyn': [13559],\n",
       "             'mathilda': [13565],\n",
       "             'tribeca': [13573],\n",
       "             'kosugi': [13574],\n",
       "             'pym': [13576],\n",
       "             'fujimoto': [13600],\n",
       "             'shreck': [13603],\n",
       "             'boreanaz': [13605],\n",
       "             'mancuso': [13625],\n",
       "             'tucci': [13633],\n",
       "             'gorehounds': [13644],\n",
       "             'thomerson': [13660],\n",
       "             'denholm': [13666],\n",
       "             'nefer': [13668],\n",
       "             'pendleton': [13674],\n",
       "             'heigl': [13675],\n",
       "             'breckinridge': [13710],\n",
       "             'mazzello': [13714],\n",
       "             'domergue': [13722],\n",
       "             'robby': [13723],\n",
       "             'darkwolf': [13728],\n",
       "             'todesking': [13729],\n",
       "             'oadly': [13732],\n",
       "             'luciano': [13741],\n",
       "             'hellman': [13747],\n",
       "             'suyin': [13749],\n",
       "             'tibbs': [13753],\n",
       "             'darlene': [13755],\n",
       "             'suzumiya': [13758],\n",
       "             'martel': [13759],\n",
       "             'astrid': [13761],\n",
       "             'parsifal': [13765],\n",
       "             'ouimet': [13768],\n",
       "             'trivialboring': [13770],\n",
       "             'chayefsky': [13773],\n",
       "             'ophelia': [13784],\n",
       "             'oshii': [13788],\n",
       "             'lorna': [13790],\n",
       "             'prochnow': [13804],\n",
       "             'whalley': [13809],\n",
       "             'pabst': [13810],\n",
       "             'stormare': [13822],\n",
       "             'goony': [13823],\n",
       "             'mccord': [13826],\n",
       "             'oker': [13830],\n",
       "             'scatman': [13833],\n",
       "             'miyagi': [13840],\n",
       "             'janine': [13846],\n",
       "             'breslin': [13850],\n",
       "             'margret': [13868],\n",
       "             'horrigan': [13884],\n",
       "             'alda': [13901],\n",
       "             'jarman': [13916],\n",
       "             'eakout': [13917],\n",
       "             'milverton': [13924],\n",
       "             'ielle': [13932],\n",
       "             'wilford': [13934],\n",
       "             'droppingly': [13937],\n",
       "             'desdemona': [13943],\n",
       "             'pando': [13946],\n",
       "             'charly': [13950],\n",
       "             'fingersmith': [13955],\n",
       "             'idged': [13963],\n",
       "             'cybill': [13965],\n",
       "             'orry': [13971],\n",
       "             'akshaye': [13993],\n",
       "             'mchugh': [13995],\n",
       "             'vipul': [14012],\n",
       "             'dilapidate': [14017],\n",
       "             'anches': [14022],\n",
       "             'stifler': [14034],\n",
       "             'noriko': [14062],\n",
       "             'scalise': [14066],\n",
       "             'xica': [14075],\n",
       "             'angelopoulos': [14086],\n",
       "             'arbus': [14087],\n",
       "             'winfrey': [14093],\n",
       "             'boman': [14097],\n",
       "             'mcgowan': [14104],\n",
       "             'saget': [14116],\n",
       "             'crothers': [14132],\n",
       "             'kershner': [14137],\n",
       "             'millard': [14144],\n",
       "             'sakall': [14147],\n",
       "             'hyeon': [14151],\n",
       "             'bueller': [14164],\n",
       "             'amenÃ£': [14174],\n",
       "             'ceylan': [14186],\n",
       "             'elam': [14187],\n",
       "             'lynchian': [14188],\n",
       "             'girlfight': [14200],\n",
       "             'resnais': [14202],\n",
       "             'benicio': [14225],\n",
       "             'smatter': [14239],\n",
       "             'balzac': [14277],\n",
       "             'weisz': [14279],\n",
       "             'reinhold': [14292],\n",
       "             'macleane': [14296],\n",
       "             'utish': [14307],\n",
       "             'burtynsky': [14311],\n",
       "             'dominoe': [14316],\n",
       "             'shefali': [14331],\n",
       "             'rampling': [14337],\n",
       "             ...})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words_not_found  #words not found in Glove embedding; CAN USE WORDCLOUD\n",
    "words_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027\n"
     ]
    }
   ],
   "source": [
    "print(len(words_not_found))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20001, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape # Embedding layer matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW HANDLE OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_to_OOV = []\n",
    "\n",
    "def checkCosineDist(lst):\n",
    "    words_not_found =lst\n",
    "    for word,i in words_not_found:    \n",
    "        try:                \n",
    "            thr = FASTEXT_Embedding.wv.most_similar_cosmul(word) # model.wv.most_similar(word)\n",
    "            print(thr[0][1], \"  \", thr[0][0])        \n",
    "\n",
    "            if thr[0][1] > 0.48:                            # if word found and is abv threashold then append thr[0][0] i.e name       \n",
    "                similar_to_OOV.append(thr[0][0])  \n",
    "                #HERE WE CAN DIRECTLY add this vector value in fastext embedding\n",
    "                #\n",
    "            else:                                               # if found with below threashold           \n",
    "                similar_to_OOV.append('<unk>')      \n",
    "\n",
    "\n",
    "        except:                      # if no mkatch found\n",
    "            similar_to_OOV.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'dogge' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-bf6e28ef5f01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mFASTEXT_Embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar_cosmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dogge'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar_cosmul\u001b[1;34m(self, positive, negative, topn)\u001b[0m\n\u001b[0;32m    829\u001b[0m         positive = [\n\u001b[0;32m    830\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpositive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m         ]\n\u001b[0;32m    833\u001b[0m         negative = [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    829\u001b[0m         positive = [\n\u001b[0;32m    830\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpositive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m         ]\n\u001b[0;32m    833\u001b[0m         negative = [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    465\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 467\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'dogge' not in vocabulary\""
     ]
    }
   ],
   "source": [
    " FASTEXT_Embedding.most_similar_cosmul('dogge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Truncating Data\n",
    "* To feed it to N.N, inputs to have the same length\n",
    " - Either we ensure that all sequences in the entire data-set have the same length\n",
    " - Or Entier batch should be of same length\n",
    "* Going about choosing ampunt to pad\n",
    " - going with longest seq, would be just waste of memory for texr whose length is small\n",
    " - going with smalles seq , would be just ignoring other imp values \n",
    " - so we go optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    8  1337    13    25   103  1709    14    92     1  3217 12926   106\n",
      "    14   199   335     1    52   294  3589  2928   240    14   289   744\n",
      "   199   335     1    52   294     1   110    32  6941   151   396    76\n",
      "    25   249   224   148   124    24  2090  2222   152   494   585    81\n",
      "     2    92    95  1189   879   157   131   195   940    92    59    35\n",
      "   186   340  4598  1407   322    72   157   346   232   732   229   158\n",
      "    88  1401    81 11214  2969   373   246    21     2  1419    28   336\n",
      "     9    91  1385   128    92   184  2969   543   100  9786  3569   903\n",
      "  6848   577   244    28     9    32  1435    32     1   349  5264  2426\n",
      "  1293   147  1410  2506  2792   515    92    32 12926   106   148   444\n",
      "   182  1368    25     2  1934   384   232  1865   909   508  1979 14507\n",
      "  2608    55  1443   603   232  3031   199   335  7292]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "All that the Embedding layer does is to map the integer inputs to the vectors found at the corresponding index in the \n",
    "embedding matrix, i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]\n",
    "'''\n",
    "print(np.array(seq[506]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14507"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.array(seq[506]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and trucating here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data_pad = sequence.pad_sequences(seq , maxlen=INPUT_TEXT_LENGTH,padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49999, 294)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data_pad.shape\n",
    "# 2k review\n",
    "# and 695 fixed I/P shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking\n",
    "#imdb_data_pad[4]\n",
    "type(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x238f26b0048>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Inverse Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good lord go say right bat watch minute movie hardcore eraserhead fan watch black white movie little dialogue defense apply simply watch terrible weird black white movie little dialogue movie happen give goth child talent nothing say camera budget let put much offensive imagery screen possible clear start film minute long assume exist shot last second drag minute director love sound voice syndrome refuse cut another shot entire piece footage view moment girl mask start masturbate corpse god open scene film joy know matter time turn tape least minute different corpse pull around twitch rope gang cloak mystery men know time give rarely give movie sit entirety blair witch book shadow albeit happily deserve minute give eraserhead fan let simple mind comparison say film con rent piece amateur trash allow refer tetsuo iron man watchable enjoyable piece incoherent black white weirdness'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(seq[506])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91671"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N.N Model\n",
    "len(tokenizer.word_index) # This are total word in dict\n",
    "#it depends if i want to use entier dict or only few occuring word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data to test/ train / dev (disabled for train test split )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= imdb_data_pad #PADDED VERSION OF DATA\n",
    "y= df['sentiment'] # LABELS OF DATA\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.30,stratify=y, random_state=42)                #30%\n",
    "#  #4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reason for starified split:**\n",
    "https://stats.stackexchange.com/questions/250273/benefits-of-stratified-vs-random-sampling-for-generating-training-data-in-classi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stratiy on y_train data\n",
    "X_train,X_test,y_train,y_test = train_test_split( X_train, y_train, test_size=0.04,stratify=y_train, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 33599    33599\n",
      "Validation data: 15000    15000\n",
      "Test data: 1400    1400\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data:\",len(X_train),'  ',len(y_train))\n",
    "print(\"Validation data:\",len(X_val),'  ',len(y_val))\n",
    "print(\"Test data:\",len(X_test),'  ',len(y_test))\n",
    "print(type(y_train))\n",
    "\n",
    "X_train = np.asarray(X_train)\n",
    "X_val = np.asarray(X_val)\n",
    "X_test = np.asarray(X_test)\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_val = np.asarray(y_val)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[155]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING A MODEL\n",
    "* http://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 294, 300)          6000300   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 294, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 6,160,801\n",
      "Trainable params: 6,160,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EMBEDDING_DIM = 300 #or 150\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=INPUT_TEXT_LENGTH))\n",
    "\n",
    "embedding_layer = Embedding(VOCAB_SIZE+1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=INPUT_TEXT_LENGTH, trainable=True)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# simple early stopping\n",
    "ec = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = ModelCheckpoint(MODEL_FILEPATH, monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "#mc = ModelCheckpoint(PROJ_NAME,'_best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "'''\n",
    "embedding_layer = Embedding(VOCAB_SIZE, 50, weights=[embedding_matrix], input_length=INPUT_TEXT_LENGTH, trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "'''\n",
    "'''model.add(CuDNNGRU(units=8, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(CuDNNGRU(units=4))'''\n",
    "\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print out model image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "import os\n",
    "#install graph viz locally 1st\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'# install \n",
    "plot_model(model, to_file=IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fitting model with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33599 samples, validate on 15000 samples\n",
      "Epoch 1/4\n",
      " - 812s - loss: 0.3982 - acc: 0.8274 - val_loss: 0.2953 - val_acc: 0.8789\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.87893, saving model to model_tokenizers_weights_and_json/2.1 Fastext_pretrain_300dims_20k_drop_LSTM_train_best_model.h5\n",
      "Epoch 2/4\n",
      " - 823s - loss: 0.2481 - acc: 0.9048 - val_loss: 0.3226 - val_acc: 0.8719\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.87893\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.299462). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.577922). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.690563). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.701630). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.701630). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.701630). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.689534). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.692723). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.682988). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.678467). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.675710). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "C:\\Users\\DBCE\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (1.641400). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 810s - loss: 0.1927 - acc: 0.9269 - val_loss: 0.4100 - val_acc: 0.8201\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.87893\n",
      "Epoch 4/4\n",
      " - 874s - loss: 0.1568 - acc: 0.9429 - val_loss: 0.3434 - val_acc: 0.8791\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.87893 to 0.87913, saving model to model_tokenizers_weights_and_json/2.1 Fastext_pretrain_300dims_20k_drop_LSTM_train_best_model.h5\n",
      "Epoch 00004: early stopping\n",
      "Wall time: 55min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(X_train, y_train , epochs=4 , validation_data=(X_val, y_val),verbose=2, batch_size = 64, callbacks=[tensorboard, ec, mc])\n",
    "# TENSORBOARD\n",
    "# tensorboard --logdir logs/\n",
    "# http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot train and validation loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.79%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Final evaluation of the model\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO after trainingÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export tokenizer and weights of model for production\n",
    "* If model has performed well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# saving instace so word can be converted to int tokens\n",
    "with open(TOKENIZER_FILEPATH, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving keras model n weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "'''model_json = model.to_json()\n",
    "with open(PROJ_NAME,\"_model.json\", \"w\") as json_file: #MODEL_FILEPATH\n",
    "    json_file.write(model_json)'''\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "#model.save_weights(WEIGHT_FILEPATH)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting .h5 to .pb for tflite operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'dense_1_1/Sigmoid:0' shape=(?, 1) dtype=float32>]\n",
      "[<tf.Tensor 'embedding_2_input_1:0' shape=(?, 294) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model= load_model(MODEL_FILEPATH)\n",
    "print (model.outputs)\n",
    "print(model.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 59 variables.\n",
      "INFO:tensorflow:Converted 59 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_tokenizers_weights_and_json\\\\2.1 Fastext_pretrain_300dims_20k_drop_LSTM_train.pb'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# Create, compile and train model...\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in model.outputs])\n",
    "\n",
    "\n",
    "#TFLite_FILEPATH = f'model_tokenizers_weights_and_json/{PROJ_NAME}.tflite'\n",
    "tf.train.write_graph(frozen_graph, f\"model_tokenizers_weights_and_json\", f\"{PROJ_NAME}.pb\", as_text=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_tokenizers_weights_and_json/2.1 Fastext_pretrain_300dims_20k_drop_LSTM_train_tokenizer_instance.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-e275c747e8b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0mthat\u001b[0m \u001b[0many\u001b[0m \u001b[0mone\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0munreliable\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mestimate\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mskill\u001b[0m \u001b[0mbased\u001b[0m \u001b[0mon\u001b[0m \u001b[0man\u001b[0m \u001b[0maverage\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0mruns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m '''\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTOKENIZER_FILEPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprod_instance_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_tokenizers_weights_and_json/2.1 Fastext_pretrain_300dims_20k_drop_LSTM_train_tokenizer_instance.pickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Neural networks are stochastic, they can produce different results when the same model is fit on the same data.\n",
    "This is mainly because of the random initial weights and the shuffling of patterns during mini-batch gradient descent. \n",
    "This means that any one scoring of a model is unreliable and we should estimate model skill based on an average of multiple runs.\n",
    "'''\n",
    "with open(TOKENIZER_FILEPATH, 'rb') as handle:\n",
    "    prod_instance_tokenizer = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "result = text_to_word_sequence(\"This movie really sucks! Can I get my money back please\")\n",
    "\n",
    "print(result)\n",
    "text_to_int= prod_instance.texts_to_sequences(result)\n",
    "print(text_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have to try:\n",
    "* include pre trained embedding before\n",
    "* set trainable param**\n",
    "* other architecture\n",
    "* hyperparam settings etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "'movie ups downs good stuff movie much outweighs bad good movie indeed sometimes dialogue sound light one noticed way \n",
    "set light amateur act good highly original \n",
    "storyline intense atmosphere gore factor high effect do supremely definitely worth watch maybe even must see horror gore fan'\n",
    "\n",
    "#refs\n",
    "# Delete the Keras model with these hyper-parameters from memory.\n",
    "del model\n",
    "    \n",
    "# Clear the Keras session, otherwise it will keep adding new\n",
    "# models to the same TensorFlow graph each time we create\n",
    "# a model with a different set of hyper-parameters.\n",
    "K.clear_session()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
